{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a2d5c7d-e870-44fb-92bf-e4bf39111f1c",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d3003-4fda-4582-9758-2f54ad7cb96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries used \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "## check devices\n",
    "# No idea how to implement GPU!\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "data_dir = './cifar10'\n",
    "\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)\n",
    "\n",
    "airplane_files = os.listdir(data_dir + \"/train/airplane\")\n",
    "print('No. of training examples for test image:', len(airplane_files))\n",
    "print(airplane_files[:5])\n",
    "\n",
    "ship_test_files = os.listdir(data_dir + \"/test/ship\")\n",
    "print(\"No. of test examples for ship:\", len(ship_test_files))\n",
    "print(ship_test_files[:5])\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "dataset = ImageFolder(data_dir+'/train', transform=ToTensor())\n",
    "\n",
    "img, label = dataset[0]\n",
    "print(img.shape, label)\n",
    "img\n",
    "\n",
    "#check the labels\n",
    "print(dataset.classes)\n",
    "\n",
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);\n",
    "\n",
    "val_size = 5000\n",
    "train_size = len(dataset) - val_size\n",
    "\n",
    "train_ds, val_ds = random_split(dataset, [train_size, val_size])\n",
    "len(train_ds), len(val_ds)\n",
    "\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "epochs = 10\n",
    "\n",
    "# test_loader is the portion within in test dataset\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define a simple CNN model\n",
    "class CNNNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNNet, self).__init__()\n",
    "        self.conv_layer = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "        )\n",
    "        self.fc_layer = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 8 * 8, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv_layer(x)\n",
    "        x = self.fc_layer(x)\n",
    "        return x\n",
    "\n",
    "# Define evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total, total_loss / len(data_loader)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs):\n",
    "    history = {\n",
    "        'train_loss': [],\n",
    "        'test_loss': [],\n",
    "        'train_acc': [],\n",
    "        'test_acc': []\n",
    "    }\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss, correct, total = 0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accuracy calculation\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "        train_acc = correct / total\n",
    "        test_acc, test_loss = evaluate(model, test_loader)\n",
    "\n",
    "        history['train_loss'].append(train_loss / len(train_loader))\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['train_acc'].append(train_acc)\n",
    "        history['test_acc'].append(test_acc)\n",
    "\n",
    "        print(f\"Epoch [{epoch + 1}/{epochs}], \"\n",
    "              f\"Train Loss: {train_loss / len(train_loader):.4f}, \"\n",
    "              f\"Test Loss: {test_loss:.4f}, \"\n",
    "              f\"Train Acc: {train_acc:.4f}, \"\n",
    "              f\"Test Acc: {test_acc:.4f}\")\n",
    "    return history\n",
    " \n",
    "from torchsummary import summary\n",
    "# Summarize the model\n",
    "print(summary(model, (3, 32, 32)))\n",
    "\n",
    "# Train the model\n",
    "history = train_model(model, train_loader, test_loader, epochs=10)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Plot the training and test accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['test_acc'], label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0517cccd-97e5-405b-abba-179aa3978346",
   "metadata": {},
   "source": [
    "# Task 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d014213-d161-4748-8349-57d077708035",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Paths \n",
    "dataset_path = './MSTRtest'  # Path to the dataset file\n",
    "labels_path = './MSTRtest'  # Path to the labels file\n",
    "\n",
    "# Preprocess data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Custom CNN Model\n",
    "class MSTRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MSTRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "num_classes = len(dataset.classes)\n",
    "model = MSTRCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "# Compute F1-score\n",
    "average_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f\"Average F1-Score: {average_f1}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
