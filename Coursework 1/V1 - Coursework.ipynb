{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2ubsJ9reMY"
   },
   "source": [
    "# MECH 3465 Robotics & Machine Intelligence - Coursework 1\n",
    "\n",
    "Dan Nehushtan - 201594650\n",
    "<br> Louie Burns - 201588498\n",
    "\n",
    "### Introduction\n",
    "This coursework applies Convolutional Neural Networks (CNNs) to classify aircraft in remote sensing images, optimising performance through hyperparameter tuning and pre-processing. The evaluation uses the weighted F1-score to assess model improvements.\n",
    "\n",
    "### Tasks\n",
    "1. Design a convolutional neural network (CNN) for aircraft classification, justifying its suitability for the task.\n",
    "2. Document the hyperparameter tuning process and analyse its impact on model performance.\n",
    "3. Optimise the CNN through pre-processing techniques or alternative network structures, using weighted F1-score for evaluation.\n",
    "4. Provide a detailed evaluation, comparing the initial and optimised models, including metrics and a critical reflection on results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Task 1 - CNN\n",
    "In this task you wil train the Aircraft Recognition Dataset using pytorch and train a Convolutional Neural Network (CNN) to classify the Aircraft Recognition Dataset using weighted F1 score. You should train the model using a GPU if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and read the folder (inluding the text file titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l7lSeJxxreFe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (3.2.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (18.1.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (0.27.1)\n",
      "Requirement already satisfied: packaging in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: colorama in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: pandas in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (2.2.2)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\dnehu\\documents\\university\\programs\\anaconda\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# INSTALL MISSING LIBRARIES (This depends on device and GPU used - some stuff may already be satisfied, some may not)\n",
    "%pip install datasets\n",
    "%pip install pandas\n",
    "%pip install scikit-learn\n",
    "\n",
    "#Import libraries used \n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import tarfile\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import random_split, Dataset, DataLoader\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import ToTensor\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the device and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Using CPU.\n"
     ]
    }
   ],
   "source": [
    "## check devices\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# If CUDA is available, print the GPU name and the device being used\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(device)\n",
    "    print(f\"CUDA is available. Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")\n",
    "\n",
    "# print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPkgv_ROR3ZX"
   },
   "source": [
    "## Load Data Set\n",
    "*   Load the AircraftRecognition Dataset\n",
    "*   Split into training and testing segments\n",
    "  * The training segment is used for training the model, while the testing portion of the data is used to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is extracted to the directory /AircraftRecognitionDataset. You need to create a local dirctory data folder. It contains 1 folder, images, containing all train and test set (10,000 images). Also it contains 19 text files which categroise the images. Let's verify this using os.listdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'AircraftRecognitionDataset.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(extracted_folder_path)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Extract the zip file\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_file_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zip_ref:\n\u001b[1;32m     13\u001b[0m     zip_ref\u001b[38;5;241m.\u001b[39mextractall(extracted_folder_path)\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mZip file extracted to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mextracted_folder_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/zipfile.py:1295\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1294\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1295\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m io\u001b[38;5;241m.\u001b[39mopen(file, filemode)\n\u001b[1;32m   1296\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'AircraftRecognitionDataset.zip'"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Define the path to the zip file\n",
    "zip_file_path = 'AircraftRecognitionDataset.zip'\n",
    "extracted_folder_path = 'AircraftRecognitionDataset'  # Folder where you want to extract\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(extracted_folder_path):\n",
    "    os.makedirs(extracted_folder_path)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "print(f'Zip file extracted to: {extracted_folder_path}')\n",
    "\n",
    "# Set the path to the extracted 'dataoriginal' folder\n",
    "data_dir = os.path.join(extracted_folder_path, 'dataoriginal')  # Path to the dataset folder inside extracted\n",
    "\n",
    "# List the text files in 'dataoriginal'\n",
    "txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "print(\"Text files found:\", txt_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_VPcGAfSfm5"
   },
   "source": [
    "## Load Dataset\n",
    "We can use the ImageFolder class from torchvision to load the data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [00:00<00:00, 93503.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Data: (3334, 2)\n",
      "Validation Data: (3333, 2)\n",
      "Test Data: (3333, 2)\n",
      "\n",
      "DatasetDict:\n",
      "Split        Features             Num Rows  \n",
      "------------ -------------------- ----------\n",
      "train        {'image': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)} 3334      \n",
      "validation   {'image': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)} 3333      \n",
      "test         {'image': Value(dtype='string', id=None), 'label': Value(dtype='string', id=None)} 3333      \n",
      "\n",
      "Families, Manufacturers, Variants:\n",
      "File            First 5 Entries                                   \n",
      "--------------- --------------------------------------------------\n",
      "Families        ['A300', 'A310', 'A320', 'A330', 'A340']\n",
      "Manufacturers   ['ATR', 'Airbus', 'Antonov', 'Beechcraft', 'Boeing']\n",
      "Variants        ['707-320', '727-200', '737-200', '737-300', '737-400']\n",
      "\n",
      "Test:\n",
      "File                 First 5 Entries                                   \n",
      "-------------------- --------------------------------------------------\n",
      "1. Family               [('1514522', 'Boeing 707'), ('0747566', 'Boeing 707'), ('1008575', 'Boeing 707'), ('0717480', 'Boeing 707'), ('0991569', 'Boeing 707')]\n",
      "2. Manufacturer         [('1514522', 'Boeing'), ('0747566', 'Boeing'), ('1008575', 'Boeing'), ('0717480', 'Boeing'), ('0991569', 'Boeing')]\n",
      "3. Variant              [('1514522', '707-320'), ('0747566', '707-320'), ('1008575', '707-320'), ('0717480', '707-320'), ('0991569', '707-320')]\n",
      "4. Image IDs            ['1514522', '0747566', '1008575', '0717480', '0991569']\n",
      "\n",
      "Train:\n",
      "File                 First 5 Entries                                   \n",
      "-------------------- --------------------------------------------------\n",
      "1. Family               [('1025794', 'Boeing 707'), ('1340192', 'Boeing 707'), ('0056978', 'Boeing 707'), ('0698580', 'Boeing 707'), ('0450014', 'Boeing 707')]\n",
      "2. Manufacturer         [('1025794', 'Boeing'), ('1340192', 'Boeing'), ('0056978', 'Boeing'), ('0698580', 'Boeing'), ('0450014', 'Boeing')]\n",
      "3. Variant              [('1025794', '707-320'), ('1340192', '707-320'), ('0056978', '707-320'), ('0698580', '707-320'), ('0450014', '707-320')]\n",
      "4. Image IDs            ['1025794', '1340192', '0056978', '0698580', '0450014']\n",
      "\n",
      "Validation:\n",
      "File                 First 5 Entries                                   \n",
      "-------------------- --------------------------------------------------\n",
      "1. Family               [('0481847', 'Boeing 707'), ('0810608', 'Boeing 707'), ('1514481', 'Boeing 707'), ('0887066', 'Boeing 707'), ('1318819', 'Boeing 707')]\n",
      "2. Manufacturer         [('0481847', 'Boeing'), ('0810608', 'Boeing'), ('1514481', 'Boeing'), ('0887066', 'Boeing'), ('1318819', 'Boeing')]\n",
      "3. Variant              [('0481847', '707-320'), ('0810608', '707-320'), ('1514481', '707-320'), ('0887066', '707-320'), ('1318819', '707-320')]\n",
      "4. Image IDs            ['0481847', '0810608', '1514481', '0887066', '1318819']\n",
      "\n",
      "Trainval:\n",
      "File                 First 5 Entries                                   \n",
      "-------------------- --------------------------------------------------\n",
      "1. Family               [('1025794', 'Boeing 707'), ('1340192', 'Boeing 707'), ('0056978', 'Boeing 707'), ('0698580', 'Boeing 707'), ('0450014', 'Boeing 707')]\n",
      "2. Manufacturer         [('1025794', 'Boeing'), ('1340192', 'Boeing'), ('0056978', 'Boeing'), ('0698580', 'Boeing'), ('0450014', 'Boeing')]\n",
      "3. Variant              [('1025794', '707-320'), ('1340192', '707-320'), ('0056978', '707-320'), ('0698580', '707-320'), ('0450014', '707-320')]\n",
      "\n",
      "Box:\n",
      "File                 First 5 Entries                                   \n",
      "-------------------- --------------------------------------------------\n",
      "1. Box                      [('1025794', '3 144 998 431'), ('0481847', '73 220 1198 508'), ('1514522', '7 217 1196 551'), ('1340192', '83 155 964 462'), ('0810608', '19 146 986 443')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the base directory path\n",
    "base_dir = Path('./AircraftRecognitionDataset/dataoriginal')\n",
    "\n",
    "# Initialize empty lists to store file names and labels for training, validation, and testing\n",
    "train_file_names, train_labels = [], []\n",
    "val_file_names, val_labels = [], []\n",
    "test_file_names, test_labels = [], []\n",
    "\n",
    "# Define the directory path for images\n",
    "directory_path = base_dir / 'images'\n",
    "\n",
    "# Define the paths to the text files containing the label to manufacturer mapping\n",
    "mapping_files = {\n",
    "    \"train\": base_dir / 'images_manufacturer_train.txt',\n",
    "    \"val\": base_dir / 'images_manufacturer_val.txt',\n",
    "    \"test\": base_dir / 'images_manufacturer_test.txt'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store the mappings\n",
    "label_to_manufacturer = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "# Read the mappings from the text files\n",
    "for split, file_path in mapping_files.items():\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            label = parts[0]\n",
    "            manufacturer = ' '.join(parts[1:])\n",
    "            label_to_manufacturer[split][label] = manufacturer\n",
    "\n",
    "# Iterate through all image files in the specified directory\n",
    "for file in tqdm(sorted(directory_path.glob('*.*g'))):  # Adjust the pattern if needed\n",
    "    label = file.stem  # Extract the label from the file name (without extension)\n",
    "    for split in [\"train\", \"val\", \"test\"]:\n",
    "        if label in label_to_manufacturer[split]:\n",
    "            manufacturer = label_to_manufacturer[split].get(label, 'Unknown')\n",
    "            eval(f\"{split}_labels\").append(manufacturer)\n",
    "            eval(f\"{split}_file_names\").append(str(file))\n",
    "\n",
    "# Create pandas dataframes from the collected file names and labels\n",
    "train_df = pd.DataFrame({\"image\": train_file_names, \"label\": train_labels})\n",
    "val_df = pd.DataFrame({\"image\": val_file_names, \"label\": val_labels})\n",
    "test_df = pd.DataFrame({\"image\": test_file_names, \"label\": test_labels})\n",
    "\n",
    "# Print dataframe shapes\n",
    "print(f\"Train Data: {train_df.shape}\")\n",
    "print(f\"Validation Data: {val_df.shape}\")\n",
    "print(f\"Test Data: {test_df.shape}\")\n",
    "\n",
    "# Combine the dataframes into a single dataset\n",
    "dataset_dict = DatasetDict({\n",
    "    'train': Dataset.from_pandas(train_df),\n",
    "    'validation': Dataset.from_pandas(val_df),\n",
    "    'test': Dataset.from_pandas(test_df)\n",
    "})\n",
    "\n",
    "# Print the dataset in a table format\n",
    "print(\"\\nDatasetDict:\")\n",
    "print(f\"{'Split':<12} {'Features':<20} {'Num Rows':<10}\")\n",
    "print(f\"{'-'*12} {'-'*20} {'-'*10}\")\n",
    "for split, dataset in dataset_dict.items():\n",
    "    print(f\"{split:<12} {str(dataset.features):<20} {dataset.num_rows:<10}\")\n",
    "\n",
    "# Load in the families, manufacturers, variants files\n",
    "files = {\n",
    "    \"families\": base_dir / 'families.txt',\n",
    "    \"manufacturers\": base_dir / 'manufacturers.txt',\n",
    "    \"variants\": base_dir / 'variants.txt'\n",
    "}\n",
    "\n",
    "# Initialize lists to store the contents\n",
    "contents = {key: [] for key in files}\n",
    "\n",
    "# Read the files\n",
    "for key, file_path in files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        contents[key] = [line.strip() for line in f]\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nFamilies, Manufacturers, Variants:\")\n",
    "print(f\"{'File':<15} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*15} {'-'*50}\")\n",
    "for key in contents:\n",
    "    print(f\"{key.capitalize():<15} {contents[key][:5]}\")\n",
    "\n",
    "# Load in all the test files\n",
    "test_files = {\n",
    "    \"family\": base_dir / 'images_family_test.txt',\n",
    "    \"manufacturer\": base_dir / 'images_manufacturer_test.txt',\n",
    "    \"test\": base_dir / 'images_test.txt',\n",
    "    \"variant\": base_dir / 'images_variant_test.txt'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store the mappings\n",
    "test_mappings = {key: {} for key in test_files if key != \"test\"}\n",
    "test_image_ids = []\n",
    "\n",
    "# Read the test files\n",
    "for key, file_path in test_files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if key == \"test\":\n",
    "            test_image_ids = [line.strip() for line in f]\n",
    "        else:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_id, label = parts\n",
    "                    test_mappings[key][image_id] = label\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nTest:\")\n",
    "print(f\"{'File':<20} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*20} {'-'*50}\")\n",
    "for i, (key, mapping) in enumerate(test_mappings.items(), 1):\n",
    "    print(f\"{i}. {key.capitalize():<20} {list(mapping.items())[:5]}\")\n",
    "print(f\"{len(test_mappings) + 1}. Image IDs {'':<10} {test_image_ids[:5]}\")\n",
    "\n",
    "# Load in all the train files\n",
    "train_files = {\n",
    "    \"family\": base_dir / 'images_family_train.txt',\n",
    "    \"manufacturer\": base_dir / 'images_manufacturer_train.txt',\n",
    "    \"train\": base_dir / 'images_train.txt',\n",
    "    \"variant\": base_dir / 'images_variant_train.txt'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store the mappings\n",
    "train_mappings = {key: {} for key in train_files if key != \"train\"}\n",
    "train_image_ids = []\n",
    "\n",
    "# Read the train files\n",
    "for key, file_path in train_files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if key == \"train\":\n",
    "            train_image_ids = [line.strip() for line in f]\n",
    "        else:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_id, label = parts\n",
    "                    train_mappings[key][image_id] = label\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nTrain:\")\n",
    "print(f\"{'File':<20} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*20} {'-'*50}\")\n",
    "for i, (key, mapping) in enumerate(train_mappings.items(), 1):\n",
    "    print(f\"{i}. {key.capitalize():<20} {list(mapping.items())[:5]}\")\n",
    "print(f\"{len(train_mappings) + 1}. Image IDs {'':<10} {train_image_ids[:5]}\")\n",
    "\n",
    "# Load in the validation (val) files\n",
    "val_files = {\n",
    "    \"family\": base_dir / 'images_family_val.txt',\n",
    "    \"manufacturer\": base_dir / 'images_manufacturer_val.txt',\n",
    "    \"val\": base_dir / 'images_val.txt',\n",
    "    \"variant\": base_dir / 'images_variant_val.txt'\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store the mappings\n",
    "val_mappings = {key: {} for key in val_files if key != \"val\"}\n",
    "val_image_ids = []\n",
    "\n",
    "# Read the val files\n",
    "for key, file_path in val_files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if key == \"val\":\n",
    "            val_image_ids = [line.strip() for line in f]\n",
    "        else:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_id, label = parts\n",
    "                    val_mappings[key][image_id] = label\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nValidation:\")\n",
    "print(f\"{'File':<20} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*20} {'-'*50}\")\n",
    "for i, (key, mapping) in enumerate(val_mappings.items(), 1):\n",
    "    print(f\"{i}. {key.capitalize():<20} {list(mapping.items())[:5]}\")\n",
    "print(f\"{len(val_mappings) + 1}. Image IDs {'':<10} {val_image_ids[:5]}\")\n",
    "\n",
    "# Load in the trainval files\n",
    "trainval_files = {\n",
    "    \"family\": base_dir / 'images_family_trainval.txt',\n",
    "    \"manufacturer\": base_dir / 'images_manufacturer_trainval.txt',\n",
    "    \"variant\": base_dir / 'images_variant_trainval.txt',\n",
    "}\n",
    "\n",
    "# Initialize dictionaries to store the mappings\n",
    "trainval_mappings = {key: {} for key in trainval_files if key != \"trainval\"}\n",
    "trainval_image_ids = []\n",
    "\n",
    "# Read the trainval files\n",
    "for key, file_path in trainval_files.items():\n",
    "    with open(file_path, \"r\") as f:\n",
    "        if key == \"trainval\":\n",
    "            trainval_image_ids = [line.strip() for line in f]\n",
    "        else:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(' ', 1)\n",
    "                if len(parts) == 2:\n",
    "                    image_id, label = parts\n",
    "                    trainval_mappings[key][image_id] = label\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nTrainval:\")\n",
    "print(f\"{'File':<20} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*20} {'-'*50}\")\n",
    "for i, (key, mapping) in enumerate(trainval_mappings.items(), 1):\n",
    "    print(f\"{i}. {key.capitalize():<20} {list(mapping.items())[:5]}\")\n",
    "\n",
    "# Load in the box file\n",
    "box_file = base_dir / 'images_box.txt'\n",
    "box_mappings = {}\n",
    "\n",
    "# Read the box file\n",
    "with open(box_file, \"r\") as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(' ', 1)\n",
    "        if len(parts) == 2:\n",
    "            image_id, box = parts\n",
    "            box_mappings[image_id] = box\n",
    "\n",
    "# Print the first few entries to verify\n",
    "print(\"\\nBox:\")\n",
    "print(f\"{'File':<20} {'First 5 Entries':<50}\")\n",
    "print(f\"{'-'*20} {'-'*50}\")\n",
    "print(f\"1. Box {'':<20} {list(box_mappings.items())[:5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bounding box logic:\n",
    "\n",
    "We have text files that map each image ID to a bounding box. We want to use just the cropped region, therefore:\n",
    "*   Parse the bounding box (e.g., x1, y1, x2, y2).\n",
    "*   Open the image and crop based on those coordinates before feeding it to the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the input data shape and labels.  Each element is a tuple, containing an image tensor and a label. Since the data consists of 256x256 px color images with 3 channels (RGB), each image tensor has the shape (3, 256, 256)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               image              label  \\\n",
      "0  AircraftRecognitionDataset\\dataoriginal\\images...  McDonnell Douglas   \n",
      "1  AircraftRecognitionDataset\\dataoriginal\\images...             Boeing   \n",
      "2  AircraftRecognitionDataset\\dataoriginal\\images...           Canadair   \n",
      "3  AircraftRecognitionDataset\\dataoriginal\\images...         Beechcraft   \n",
      "4  AircraftRecognitionDataset\\dataoriginal\\images...             Boeing   \n",
      "\n",
      "  image_id   x1   y1   x2   y2  label_int  \n",
      "0  0038598  117    2  969  724         21  \n",
      "1  0038671    1  113  743  526          4  \n",
      "2  0043892   25  115  969  726          7  \n",
      "3  0048340   42  115  936  393          3  \n",
      "4  0054367    2  180  839  538          4  \n"
     ]
    }
   ],
   "source": [
    "# 1) Parse bounding boxes:\n",
    "parsed_box_mappings = {}\n",
    "for img_id, coords_str in box_mappings.items():\n",
    "    coords = list(map(int, coords_str.split()))\n",
    "    if len(coords) == 4:\n",
    "        x1, y1, x2, y2 = coords\n",
    "        parsed_box_mappings[img_id] = (x1, y1, x2, y2)\n",
    "\n",
    "def add_box_data(df):\n",
    "    df['image_id'] = df['image'].apply(lambda p: Path(p).stem)\n",
    "    df[['x1','y1','x2','y2']] = df['image_id'].apply(\n",
    "        lambda i: pd.Series(parsed_box_mappings.get(i, (0,0,0,0)))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "train_df = add_box_data(train_df)\n",
    "val_df   = add_box_data(val_df)\n",
    "test_df  = add_box_data(test_df)\n",
    "\n",
    "# 2) Combine label info from \"trainval\" file:\n",
    "# Example: use trainval_mappings[\"manufacturer\"] to gather all labels\n",
    "all_manufacturers = list(trainval_mappings[\"manufacturer\"].values())\n",
    "unique_manufacturers = sorted(set(all_manufacturers))\n",
    "label_map = {m: i for i, m in enumerate(unique_manufacturers)}\n",
    "\n",
    "# 3) Convert string labels to numeric in each DataFrame:\n",
    "train_df['label_int'] = train_df['label'].map(label_map)\n",
    "val_df['label_int']   = val_df['label'].map(label_map)\n",
    "test_df['label_int']  = test_df['label'].map(label_map)\n",
    "\n",
    "print(train_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mDWS9zwsG1x"
   },
   "source": [
    "## Preprocess the dataset and augment\n",
    "Preprocess the dataset by normalizing the pixel values between 0 and 1. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "L-yaiJJDsz2h"
   },
   "outputs": [],
   "source": [
    "random_seed = 42\n",
    "torch.manual_seed(random_seed);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'int' and 'str'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:218\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 218\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py:242\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(op, a, b, use_numexpr)\u001b[0m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_numexpr:\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;66;03m# error: \"None\" not callable\u001b[39;00m\n\u001b[0;32m--> 242\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_str\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _evaluate_standard(op, op_str, a, b)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/computation/expressions.py:73\u001b[0m, in \u001b[0;36m_evaluate_standard\u001b[0;34m(op, op_str, a, b)\u001b[0m\n\u001b[1;32m     72\u001b[0m     _store_test_result(\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m---> 73\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/roperator.py:15\u001b[0m, in \u001b[0;36mrsub\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrsub\u001b[39m(left, right):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mright\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'str'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m train_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_dict\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mval_df\u001b[49m\n\u001b[1;32m      3\u001b[0m train_ds, val_ds \u001b[38;5;241m=\u001b[39m random_split(dataset_dict, [train_size, val_df])\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mlen\u001b[39m(train_ds), \u001b[38;5;28mlen\u001b[39m(val_ds)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/common.py:76\u001b[0m, in \u001b[0;36m_unpack_zerodim_and_defer.<locals>.new_method\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mNotImplemented\u001b[39m\n\u001b[1;32m     74\u001b[0m other \u001b[38;5;241m=\u001b[39m item_from_zerodim(other)\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmethod\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mother\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/arraylike.py:198\u001b[0m, in \u001b[0;36mOpsMixin.__rsub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;129m@unpack_zerodim_and_defer\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__rsub__\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    197\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__rsub__\u001b[39m(\u001b[38;5;28mself\u001b[39m, other):\n\u001b[0;32m--> 198\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arith_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mroperator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrsub\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:7913\u001b[0m, in \u001b[0;36mDataFrame._arith_method\u001b[0;34m(self, other, op)\u001b[0m\n\u001b[1;32m   7910\u001b[0m \u001b[38;5;28mself\u001b[39m, other \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_align_for_op(other, axis, flex\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, level\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m   7912\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m np\u001b[38;5;241m.\u001b[39merrstate(\u001b[38;5;28mall\u001b[39m\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 7913\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dispatch_frame_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mother\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7914\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_construct_result(new_data)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/frame.py:7945\u001b[0m, in \u001b[0;36mDataFrame._dispatch_frame_op\u001b[0;34m(self, right, func, axis)\u001b[0m\n\u001b[1;32m   7942\u001b[0m right \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mitem_from_zerodim(right)\n\u001b[1;32m   7943\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_list_like(right):\n\u001b[1;32m   7944\u001b[0m     \u001b[38;5;66;03m# i.e. scalar, faster than checking np.ndim(right) == 0\u001b[39;00m\n\u001b[0;32m-> 7945\u001b[0m     bm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_op\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7946\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(bm, axes\u001b[38;5;241m=\u001b[39mbm\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   7948\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(right, DataFrame):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/managers.py:361\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    358\u001b[0m             kwargs[k] \u001b[38;5;241m=\u001b[39m obj[b\u001b[38;5;241m.\u001b[39mmgr_locs\u001b[38;5;241m.\u001b[39mindexer]\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(f):\n\u001b[0;32m--> 361\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[43mb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    362\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    363\u001b[0m     applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(b, f)(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:283\u001b[0m, in \u001b[0;36marithmetic_op\u001b[0;34m(left, right, op)\u001b[0m\n\u001b[1;32m    279\u001b[0m     _bool_arith_check(op, left, right)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    281\u001b[0m     \u001b[38;5;66;03m# error: Argument 1 to \"_na_arithmetic_op\" has incompatible type\u001b[39;00m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;66;03m# \"Union[ExtensionArray, ndarray[Any, Any]]\"; expected \"ndarray[Any, Any]\"\u001b[39;00m\n\u001b[0;32m--> 283\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[43m_na_arithmetic_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    285\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m res_values\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:227\u001b[0m, in \u001b[0;36m_na_arithmetic_op\u001b[0;34m(left, right, op, is_cmp)\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_cmp \u001b[38;5;129;01mand\u001b[39;00m (\n\u001b[1;32m    221\u001b[0m         left\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(right, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m\n\u001b[1;32m    222\u001b[0m     ):\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    225\u001b[0m         \u001b[38;5;66;03m# Don't do this for comparisons, as that will handle complex numbers\u001b[39;00m\n\u001b[1;32m    226\u001b[0m         \u001b[38;5;66;03m#  incorrectly, see GH#32047\u001b[39;00m\n\u001b[0;32m--> 227\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43m_masked_arith_op\u001b[49m\u001b[43m(\u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    228\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    229\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/ops/array_ops.py:182\u001b[0m, in \u001b[0;36m_masked_arith_op\u001b[0;34m(x, y, op)\u001b[0m\n\u001b[1;32m    179\u001b[0m         mask \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mwhere(y \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m, mask)\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m--> 182\u001b[0m         result[mask] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mxrav\u001b[49m\u001b[43m[\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m np\u001b[38;5;241m.\u001b[39mputmask(result, \u001b[38;5;241m~\u001b[39mmask, np\u001b[38;5;241m.\u001b[39mnan)\n\u001b[1;32m    185\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape)  \u001b[38;5;66;03m# 2D compat\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/pandas/core/roperator.py:15\u001b[0m, in \u001b[0;36mrsub\u001b[0;34m(left, right)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrsub\u001b[39m(left, right):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mright\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mleft\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'int' and 'str'"
     ]
    }
   ],
   "source": [
    "train_size = len(dataset_dict) - val_df\n",
    "\n",
    "train_ds, val_ds = random_split(dataset_dict, [train_size, val_df])\n",
    "len(train_ds), len(val_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create data loaders for training and validation, to load the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.dataloader import DataLoader\n",
    "\n",
    "batch_size=128\n",
    "epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_loader is the portion within in test dataset\n",
    "train_loader = DataLoader(train_ds, batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(val_ds, batch_size*2, num_workers=4, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the number of classes (e.g., number of manufacturers)\n",
    "num_classes = len(set([label[0] for label in train_dataset.image_labels.values()]))\n",
    "model = SimpleCNN(num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, data_loader):\n",
    "    model.eval()\n",
    "    total, correct, total_loss = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    return correct / total, total_loss / len(data_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNNNet().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model and keep track of the loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Collect all unique labels\u001b[39;00m\n\u001b[1;32m      2\u001b[0m all_labels \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m labels \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtrain_ds\u001b[49m\u001b[38;5;241m.\u001b[39mimage_labels\u001b[38;5;241m.\u001b[39mvalues():\n\u001b[1;32m      4\u001b[0m     all_labels\u001b[38;5;241m.\u001b[39mupdate(labels)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Create a mapping from label to index\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_ds' is not defined"
     ]
    }
   ],
   "source": [
    "# Collect all unique labels\n",
    "all_labels = set()\n",
    "for labels in train_ds.image_labels.values():\n",
    "    all_labels.update(labels)\n",
    "\n",
    "# Create a mapping from label to index\n",
    "label_to_index = {label: idx for idx, label in enumerate(sorted(all_labels))}\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, label_to_index, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.label_to_index = label_to_index\n",
    "        self.image_labels = self.load_annotations(annotations_file)\n",
    "        self.image_paths = list(self.image_labels.keys())\n",
    "\n",
    "    def load_annotations(self, annotations_file):\n",
    "        image_labels = {}\n",
    "        with open(annotations_file, 'r') as file:\n",
    "            for line in file:\n",
    "                parts = line.strip().split()\n",
    "                image_name = parts[0]\n",
    "                labels = parts[1:]  # Collect all labels (manufacturer, variant, etc.)\n",
    "                image_labels[image_name] = labels\n",
    "        return image_labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_name = self.image_paths[idx]\n",
    "        img_path = os.path.join(self.image_dir, image_name + '.jpg')  # Assuming images have .jpg extension\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        labels = self.image_labels[image_name]\n",
    "        labels = [self.label_to_index[label] for label in labels]  # Convert labels to indices\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, labels\n",
    "\n",
    "# Define the base directory path\n",
    "base_dir = Path('./AircraftRecognitionDataset/dataoriginal')\n",
    "\n",
    "# Define the paths to the text files containing the label to manufacturer mapping\n",
    "mapping_files = {\n",
    "    \"train\": base_dir / 'images_manufacturer_train.txt',\n",
    "    \"val\": base_dir / 'images_manufacturer_val.txt',\n",
    "    \"test\": base_dir / 'images_manufacturer_test.txt'\n",
    "}\n",
    "\n",
    "# Define the transformations\n",
    "transform = Compose([\n",
    "    Resize((32, 32)),  # Resize all images to 32x32\n",
    "    ToTensor(),\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalize with mean and std\n",
    "])\n",
    "\n",
    "# Create datasets for train, validation, and test splits\n",
    "train_ds = CustomImageDataset(base_dir / 'images', mapping_files['train'], label_to_index, transform=transform)\n",
    "val_ds = CustomImageDataset(base_dir / 'images', mapping_files['val'], label_to_index, transform=transform)\n",
    "test_ds = CustomImageDataset(base_dir / 'images', mapping_files['test'], label_to_index, transform=transform)\n",
    "\n",
    "# Define batch size\n",
    "batch_size = 32\n",
    "\n",
    "# Create DataLoaders for each dataset\n",
    "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n",
    "val_loader = DataLoader(val_ds, batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size*2, shuffle=False, num_workers=4, pin_memory=True)\n",
    "\n",
    "# Define the CNN model\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
    "        self.fc1 = nn.Linear(64 * 8 * 8, 512)\n",
    "        self.fc2 = nn.Linear(512, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 64 * 8 * 8)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Define the number of classes (e.g., number of manufacturers)\n",
    "num_classes = len(label_to_index)\n",
    "model = SimpleCNN(num_classes)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        labels = [label[0] for label in labels]  # Use the first label (manufacturer)\n",
    "        labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            labels = [label[0] for label in labels]  # Use the first label (manufacturer)\n",
    "            labels = torch.tensor(labels)  # Convert labels to tensor\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    print(f\"Validation Accuracy: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# Summarize the model\n",
    "print(summary(model, (3, 32, 32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history = train_model(model, train_loader, test_loader, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the training and test accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Plot the training and test accuracy and loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "plt.plot(history['test_acc'], label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Train vs Test Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Plot Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history['train_loss'], label='Train Loss')\n",
    "plt.plot(history['test_loss'], label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Train vs Test Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2\n",
    "In this task you wil train the MSTR dataset using pytorch and train a Convolutional Neural Network (CNN) to classify the MSTR dataset using weighted F1 score. You should train the model using a GPU if available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.11/site-packages (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (2.1.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.14.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/conda/lib/python3.11/site-packages (from scikit-learn) (3.5.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "!pip install scikit-learn\n",
    "from sklearn.metrics import f1_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Paths \n",
    "dataset_path = './AircraftRecognitionDataset'  # Path to the dataset file\n",
    "labels_path = './AircraftRecognitionDataset'  # Path to the labels file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess data\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize images\n",
    "    transforms.ToTensor(),  # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Load dataset\n",
    "dataset = datasets.ImageFolder(dataset_path, transform=transform)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom CNN Model\n",
    "class MSTRCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(MSTRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Adjust based on input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.conv1(x)))\n",
    "        x = self.pool(self.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(epochs):\n\u001b[1;32m     11\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpossibly_batched_index\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/datasets/folder.py:245\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    242\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    244\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[0;32m--> 245\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    247\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/datasets/folder.py:284\u001b[0m, in \u001b[0;36mdefault_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 284\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torchvision/datasets/folder.py:264\u001b[0m, in \u001b[0;36mpil_loader\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m    263\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[0;32m--> 264\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/PIL/Image.py:922\u001b[0m, in \u001b[0;36mImage.convert\u001b[0;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[1;32m    874\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[1;32m    875\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[1;32m    876\u001b[0m ):\n\u001b[1;32m    877\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    878\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[1;32m    879\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[1;32m    920\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 922\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    924\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    926\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/PIL/ImageFile.py:291\u001b[0m, in \u001b[0;36mImageFile.load\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(msg)\n\u001b[1;32m    290\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[0;32m--> 291\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize model\n",
    "num_classes = len(dataset.classes)\n",
    "model = MSTRCNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 5\n",
    "model.train()\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}\")\n",
    "\n",
    "# Evaluation loop\n",
    "model.eval()\n",
    "true_labels = []\n",
    "predicted_labels = []\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in dataloader:\n",
    "        outputs = model(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute F1-score\n",
    "average_f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "print(f\"Average F1-Score: {average_f1}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
