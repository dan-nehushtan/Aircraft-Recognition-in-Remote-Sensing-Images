{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rt2ubsJ9reMY"
   },
   "source": [
    "# MECH 3465 Robotics & Machine Intelligence - Coursework 1\n",
    "\n",
    "Dan Nehushtan - 201594650\n",
    "<br> Louie Burns - 201588498\n",
    "\n",
    "### Introduction\n",
    "This coursework applies Convolutional Neural Networks (CNNs) to classify aircraft in remote sensing images, optimising performance through hyperparameter tuning and pre-processing. The evaluation uses the weighted F1-score to assess model improvements.\n",
    "\n",
    "### Tasks\n",
    "1. Design a convolutional neural network (CNN) for aircraft classification, justifying its suitability for the task.\n",
    "2. Document the hyperparameter tuning process and analyse its impact on model performance.\n",
    "3. Optimise the CNN through pre-processing techniques or alternative network structures, using weighted F1-score for evaluation.\n",
    "4. Provide a detailed evaluation, comparing the initial and optimised models, including metrics and a critical reflection on results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Task 1 - CNN\n",
    "In this task you wil train the Aircraft Recognition Dataset using pytorch and train a Convolutional Neural Network (CNN) to classify the Aircraft Recognition Dataset using weighted F1 score. You should train the model using a GPU if available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import libraries and read the folder (inluding the text file titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "l7lSeJxxreFe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# INSTALL MISSING LIBRARIES (This depends on device and GPU used - some stuff may already be satisfied, some may not)\n",
    "%pip install datasets --quiet >NUL 2>&1\n",
    "%pip install pandas --quiet >NUL 2>&1\n",
    "%pip install scikit-learn --quiet >NUL 2>&1\n",
    "%pip install matplotlib --quiet >NUL 2>&1\n",
    "\n",
    "#Import libraries used \n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader, Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the device and load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Using GPU: NVIDIA GeForce RTX 2060 SUPER\n"
     ]
    }
   ],
   "source": [
    "## check devices\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# If CUDA is available, print the GPU name and the device being used\n",
    "if torch.cuda.is_available():\n",
    "    # Get the name of the current GPU\n",
    "    gpu_name = torch.cuda.get_device_name(device)\n",
    "    print(f\"CUDA is available. Using GPU: {gpu_name}\")\n",
    "else:\n",
    "    print(\"CUDA is not available. Using CPU.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nPkgv_ROR3ZX"
   },
   "source": [
    "## Load Data Set\n",
    "*   Load the AircraftRecognition Dataset\n",
    "*   Split into training and testing segments\n",
    "  * The training segment is used for training the model, while the testing portion of the data is used to evaluate the accuracy of the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is extracted to the directory /AircraftRecognitionDataset. You need to create a local dirctory data folder. It contains 1 folder, images, containing all train and test set (10,000 images). Also it contains 19 text files which categorise the images. Verify this using os.listdir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zip file extracted to: AircraftRecognitionDataset\n",
      "Text files found: 19\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "\n",
    "# Define the path to the zip file\n",
    "zip_file_path = 'AircraftRecognitionDataset.zip'\n",
    "extracted_folder_path = 'AircraftRecognitionDataset'  # Folder where you want to extract\n",
    "\n",
    "# Check if the folder exists, if not, create it\n",
    "if not os.path.exists(extracted_folder_path):\n",
    "    os.makedirs(extracted_folder_path)\n",
    "\n",
    "# Extract the zip file\n",
    "with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
    "    zip_ref.extractall(extracted_folder_path)\n",
    "\n",
    "print(f'Zip file extracted to: {extracted_folder_path}')\n",
    "\n",
    "# Set the path to the extracted 'dataoriginal' folder\n",
    "data_dir = os.path.join(extracted_folder_path, 'dataoriginal')  # Path to the dataset folder inside extracted\n",
    "\n",
    "# List the text files in 'dataoriginal'\n",
    "txt_files = [f for f in os.listdir(data_dir) if f.endswith(\".txt\")]\n",
    "print(\"Text files found:\", len(txt_files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W_VPcGAfSfm5"
   },
   "source": [
    "## Load Dataset\n",
    "We can use the ImageFolder class from torchvision to load the data as PyTorch tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'ATR': 0, 'Airbus': 1, 'Antonov': 2, 'Beechcraft': 3, 'Boeing': 4, 'Bombardier Aerospace': 5, 'British Aerospace': 6, 'Canadair': 7, 'Cessna': 8, 'Cirrus Aircraft': 9, 'Dassault Aviation': 10, 'Dornier': 11, 'Douglas Aircraft Company': 12, 'Embraer': 13, 'Eurofighter': 14, 'Fairchild': 15, 'Fokker': 16, 'Gulfstream Aerospace': 17, 'Ilyushin': 18, 'Lockheed Corporation': 19, 'Lockheed Martin': 20, 'McDonnell Douglas': 21, 'Panavia': 22, 'Piper': 23, 'Robin': 24, 'Saab': 25, 'Supermarine': 26, 'Tupolev': 27, 'Yakovlev': 28, 'de Havilland': 29}\n"
     ]
    }
   ],
   "source": [
    "# Define the folder containing the images\n",
    "image_folder = './AircraftRecognitionDataset/dataoriginal/images'\n",
    "\n",
    "# Paths to the text files\n",
    "train_label_file = './AircraftRecognitionDataset/dataoriginal/images_manufacturer_train.txt'\n",
    "test_label_file = './AircraftRecognitionDataset/dataoriginal/images_manufacturer_test.txt'\n",
    "val_label_file = './AircraftRecognitionDataset/dataoriginal/images_manufacturer_trainval.txt'\n",
    "\n",
    "# Define label mapping\n",
    "from collections import Counter\n",
    "\n",
    "# Initialize a set to store unique manufacturers\n",
    "manufacturers = set()\n",
    "\n",
    "# Open and read the training file\n",
    "with open('./AircraftRecognitionDataset/dataoriginal/images_manufacturer_train.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        # Split the line into two parts: image path and label\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) == 2:\n",
    "            label = parts[1].strip()\n",
    "            manufacturers.add(label)\n",
    "\n",
    "# Convert the set to a sorted list alphabetically\n",
    "manufacturers = sorted(list(manufacturers))\n",
    "\n",
    "# Validate the number of unique manufacturers\n",
    "if len(manufacturers) != 30:\n",
    "    raise ValueError(f\"Expected 30 unique manufacturers, got {len(manufacturers)}\")\n",
    "\n",
    "# Create the label mapping\n",
    "label_mapping = {m: i for i, m in enumerate(manufacturers)}\n",
    "print(label_mapping)\n",
    "\n",
    "def select_balanced_classes(train_file, val_file, test_file, label_mapping, num_classes=5):\n",
    "    def count_labels(file_path):\n",
    "        counts = Counter()\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                _, label = line.strip().split(maxsplit=1)\n",
    "                if label in label_mapping:\n",
    "                    counts[label] += 1\n",
    "        return counts\n",
    "\n",
    "    train_counts = count_labels(train_file)\n",
    "    val_counts = count_labels(val_file)\n",
    "    test_counts = count_labels(test_file)\n",
    "\n",
    "    # Combine counts for all datasets\n",
    "    combined_counts = {label: train_counts[label] + val_counts[label] + test_counts[label] for label in label_mapping}\n",
    "    \n",
    "    # Sort by total count and select top labels\n",
    "    selected_labels = dict(sorted(combined_counts.items(), key=lambda item: item[1], reverse=True)[:num_classes])\n",
    "    \n",
    "    # Map selected labels to indices\n",
    "    selected_labels = {label: idx for idx, (label, _) in enumerate(selected_labels.items())}\n",
    "\n",
    "    return selected_labels\n",
    "\n",
    "# Intelligently select classes based on data distribution\n",
    "selected_labels = select_balanced_classes(train_label_file, val_label_file, test_label_file, label_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mDWS9zwsG1x"
   },
   "source": [
    "## Preprocess the dataset and augment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preprocess the dataset by normalizing the pixel values between 0 and 1.\n",
    "\n",
    "Use 'images_box.txt' to bound the images and remove unecessary background. Essentially honing into the relevant part of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1799 images from ./AircraftRecognitionDataset/dataoriginal/images_manufacturer_train.txt\n",
      "Instance 1:\n",
      "Label: Boeing\n",
      "Bounding Box: (3, 144, 998, 431)\n",
      "Image Tensor Shape: torch.Size([3, 512, 512])\n",
      "Instance 2:\n",
      "Label: Boeing\n",
      "Bounding Box: (83, 155, 964, 462)\n",
      "Image Tensor Shape: torch.Size([3, 512, 512])\n",
      "Instance 3:\n",
      "Label: Boeing\n",
      "Bounding Box: (1, 129, 891, 436)\n",
      "Image Tensor Shape: torch.Size([3, 512, 512])\n"
     ]
    }
   ],
   "source": [
    "from torchvision.transforms.functional import crop, resize, to_tensor\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomRotation(15),      # Random rotation up to 15 degrees\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2),  # Random color jitter\n",
    "    transforms.ToTensor(),              # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize\n",
    "])\n",
    "\n",
    "def preprocess_image(image, bbox, target_size=(512, 512)):\n",
    "    # Crop the image using the bounding box\n",
    "    x_min, y_min, x_max, y_max = bbox\n",
    "    cropped_image = crop(image, y_min, x_min, y_max - y_min, x_max - x_min)\n",
    "    \n",
    "    # Resize the cropped image to the target size\n",
    "    resized_image = resize(cropped_image, target_size)\n",
    "    \n",
    "    # Apply other transformations\n",
    "    transformed_image = transform(resized_image)\n",
    "    \n",
    "    return transformed_image\n",
    "\n",
    "def filter_data(label_file, selected_labels):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            filename, label = parts\n",
    "            if label in selected_labels:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        bbox = bbox_dict.get(filename, (0, 0, image.width, image.height))\n",
    "                        image_tensor = preprocess_image(image, bbox)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(selected_labels[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Incorporate bounding boxes\n",
    "\n",
    "# Define file path\n",
    "bbox_file = './AircraftRecognitionDataset/dataoriginal/images_box.txt'\n",
    "\n",
    "def load_bounding_boxes(txt_file):\n",
    "    bbox_dict = {}\n",
    "    with open(txt_file, 'r') as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split()\n",
    "            image_id, x1, y1, x2, y2 = parts[0], int(parts[1]), int(parts[2]), int(parts[3]), int(parts[4])\n",
    "            bbox_dict[image_id] = (x1, y1, x2, y2)\n",
    "    return bbox_dict\n",
    "\n",
    "class AircraftDataset(Dataset):\n",
    "    def __init__(self, images_dir, label_file, bbox_file, selected_labels, transform=None, max_images_per_class=None):\n",
    "        self.images_dir = images_dir\n",
    "        self.transform = transform\n",
    "        self.bbox_dict = load_bounding_boxes(bbox_file)\n",
    "        self.image_data = []\n",
    "        self.labels = []\n",
    "        self.selected_labels = selected_labels\n",
    "        self.max_images_per_class = max_images_per_class\n",
    "\n",
    "        label_counts = {label: 0 for label in selected_labels}\n",
    "\n",
    "        with open(label_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split(maxsplit=1)\n",
    "                if len(parts) != 2:\n",
    "                    continue  # Skip malformed lines\n",
    "                filename, label = parts\n",
    "                if label in selected_labels and (max_images_per_class is None or label_counts[label] < max_images_per_class):\n",
    "                    image_path = os.path.join(images_dir, filename + \".jpg\")\n",
    "                    if os.path.exists(image_path):\n",
    "                        self.image_data.append((image_path, label))\n",
    "                        label_counts[label] += 1\n",
    "\n",
    "        print(f\"Loaded {len(self.image_data)} images from {label_file}\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        image_path, label = self.image_data[idx]\n",
    "        image = Image.open(image_path).convert(\"RGB\")\n",
    "        image_id = os.path.basename(image_path).split('.')[0]\n",
    "        if image_id in self.bbox_dict:\n",
    "            x1, y1, x2, y2 = self.bbox_dict[image_id]\n",
    "        else:\n",
    "            x1, y1, x2, y2 = 0, 0, image.size[0], image.size[1]  # Default to full image if bbox not found\n",
    "        image = preprocess_image(image, (x1, y1, x2, y2))\n",
    "        label = self.selected_labels[label]\n",
    "        return image, label, (x1, y1, x2, y2)\n",
    "\n",
    "# Create the train_dataset instance\n",
    "train_dataset = AircraftDataset(image_folder, train_label_file, bbox_file, selected_labels, transform)\n",
    "\n",
    "# Print the first 3 instances from the dataset\n",
    "for i in range(3):\n",
    "    image, label, bbox = train_dataset[i]\n",
    "    print(f\"Instance {i+1}:\")\n",
    "    print(f\"Label: {list(selected_labels.keys())[list(selected_labels.values()).index(label)]}\")\n",
    "    print(f\"Bounding Box: {bbox}\")\n",
    "    print(f\"Image Tensor Shape: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create data loaders for training and validation, to load the data in batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 695, 1024] at entry 0 and [3, 699, 1024] at entry 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 39\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor, label_tensor, labels\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Process training, validation, and test data for selected labels\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m train_image_tensor, train_label_tensor, train_labels \u001b[38;5;241m=\u001b[39m \u001b[43mfilter_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_label_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselected_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_images_per_class\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     40\u001b[0m val_image_tensor, val_label_tensor, val_labels \u001b[38;5;241m=\u001b[39m filter_data(val_label_file, selected_labels, max_images_per_class)\n\u001b[1;32m     41\u001b[0m test_image_tensor, test_label_tensor, test_labels \u001b[38;5;241m=\u001b[39m filter_data(test_label_file, selected_labels, max_images_per_class)\n",
      "Cell \u001b[0;32mIn[6], line 33\u001b[0m, in \u001b[0;36mfilter_data\u001b[0;34m(label_file, selected_labels, max_images_per_class)\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimage_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# Convert lists to PyTorch tensors\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m image_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstack\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m label_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(labels, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m image_tensor, label_tensor, labels\n",
      "\u001b[0;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 695, 1024] at entry 0 and [3, 699, 1024] at entry 1"
     ]
    }
   ],
   "source": [
    "# Increase the number of classes and images per class\n",
    "num_classes = 30  # Adjust this to the desired number of classes\n",
    "max_images_per_class = 1000  # Adjust this to the desired number of images per class\n",
    "\n",
    "# Intelligently select classes based on data distribution\n",
    "selected_labels = select_balanced_classes(train_label_file, val_label_file, test_label_file, label_mapping, num_classes=num_classes)\n",
    "\n",
    "# Update the filter_data function to use the new max_images_per_class parameter\n",
    "def filter_data(label_file, selected_labels, max_images_per_class):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "    label_counts = {label: 0 for label in selected_labels}\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            filename, label = parts\n",
    "            if label in selected_labels and label_counts[label] < max_images_per_class:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        image_tensor = transform(image)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(selected_labels[label])\n",
    "                        label_counts[label] += 1\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Process training, validation, and test data for selected labels\n",
    "train_image_tensor, train_label_tensor, train_labels = filter_data(train_label_file, selected_labels, max_images_per_class)\n",
    "val_image_tensor, val_label_tensor, val_labels = filter_data(val_label_file, selected_labels, max_images_per_class)\n",
    "test_image_tensor, test_label_tensor, test_labels = filter_data(test_label_file, selected_labels, max_images_per_class)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training images: {len(train_image_tensor)}\")\n",
    "print(f\"Number of validation images: {len(val_image_tensor)}\")\n",
    "print(f\"Number of test images: {len(test_image_tensor)}\")\n",
    "\n",
    "# Print counts for each class in training, validation, and test datasets\n",
    "train_counts = Counter(train_labels)\n",
    "val_counts = Counter(val_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "print(\"Training dataset class distribution:\")\n",
    "for label, count in train_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Validation dataset class distribution:\")\n",
    "for label, count in val_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Test dataset class distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_image_tensor, train_label_tensor)\n",
    "val_dataset = TensorDataset(val_image_tensor, val_label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n",
    "\n",
    "# Create datasets (calculate runtime here - takes a long time!)\n",
    "train_dataset = AircraftDataset(image_folder, train_label_file, bbox_file, selected_labels, transform, max_images_per_class=max_images_per_class)\n",
    "val_dataset = AircraftDataset(image_folder, val_label_file, bbox_file, selected_labels, transform, max_images_per_class=max_images_per_class)\n",
    "test_dataset = AircraftDataset(image_folder, test_label_file, bbox_file, selected_labels, transform, max_images_per_class=max_images_per_class)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "batch_size = 32  # Increased batch size (higher speeds up loading)\n",
    "num_workers = 4  # Increase the number of workers for parallel data loading\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)\n",
    "print(\"DataLoaders for selected labels created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        # Convolutional blocks\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        \n",
    "        # For images of size 512x512 after 2 pooling layers -> 128 x 128 x 32 = 524288\n",
    "        self.fc1 = nn.Linear(32 * 128 * 128, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, len(selected_labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(-1, 32 * 128 * 128)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the simplified model\n",
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  # Added momentum and a fixed learning rate\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)  # Decays learning rate by 0.1 every 10 epochs\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "f1_scores = []\n",
    "learning_rates = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model and keep track of the loss and accuracy over epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping and best model saving\n",
    "best_val_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "patience_limit = 5\n",
    "\n",
    "num_epochs = 50  # Increased number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for images, labels, _ in train_loader:  # Adjusted to unpack three values\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels, _ in test_loader:  # Adjusted to unpack three values\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], \"\n",
    "          f\"Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Test Loss: {test_loss/len(test_loader):.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}, \"\n",
    "          f\"Test Acc: {test_accuracy:.4f}, \"\n",
    "          f\"F1 Score: {f1:.4f}, \"\n",
    "          f\"Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "    # Save best model and check for early stopping\n",
    "    if test_accuracy > best_val_accuracy:\n",
    "        best_val_accuracy = test_accuracy\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")\n",
    "        patience_counter = 0\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience_limit:\n",
    "            print(\"Early stopping.\")\n",
    "            break\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Create Epochs for plotting results\n",
    "epochs = range(1, num_epochs + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## visualize the training and test accuracy and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Improved Model F1 Scores\n",
    "plt.plot(epochs, f1_scores, linestyle='-', label=\"Improved Model\")\n",
    "\n",
    "# Plot the F1 values from the Baseline Model, this is hard-coded in\n",
    "plt.plot(\n",
    "    range(1, 16), \n",
    "    [0.2885368466152528, 0.2885368466152528, 0.2885368466152528, 0.35800560282056604, \n",
    "     0.2885368466152528, 0.2885368466152528, 0.28990515812498047, 0.2885368466152528, \n",
    "     0.36161119202439296, 0.2913618247972723, 0.3724076408386039, 0.3755616336834515, \n",
    "     0.29490095228198265, 0.29349818381047593, 0.32165556746495966], linestyle='-', label=\"Simple Model\"\n",
    ")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [0.01] * 15\n",
    "\n",
    "# Plot improved model (all 50 epochs)\n",
    "plt.plot(range(1, num_epochs + 1), learning_rates, marker='o', linestyle='-', label=\"Improved Model\")\n",
    "# Plot baseline model (up to epoch 15)\n",
    "plt.plot(range(1, 16), baseline, linestyle='--', color='red', label=\"Baseline Model\")\n",
    "plt.title(\"Learning Rate vs. Epoch Number\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
