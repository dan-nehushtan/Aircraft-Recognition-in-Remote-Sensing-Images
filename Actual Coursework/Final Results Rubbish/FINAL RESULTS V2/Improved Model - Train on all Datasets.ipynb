{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5303d9a5-0466-4f6f-a672-418bc32e7541",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 4.6175, Test Loss: 4.5986, Train Acc: 0.0093, Test Acc: 0.0132, F1 Score: 0.0017\n",
      "Epoch [2/50], Train Loss: 4.5830, Test Loss: 4.5652, Train Acc: 0.0198, Test Acc: 0.0150, F1 Score: 0.0020\n",
      "Epoch [3/50], Train Loss: 4.5225, Test Loss: 4.5070, Train Acc: 0.0276, Test Acc: 0.0336, F1 Score: 0.0101\n",
      "Epoch [4/50], Train Loss: 4.4340, Test Loss: 4.4204, Train Acc: 0.0405, Test Acc: 0.0381, F1 Score: 0.0119\n",
      "Epoch [5/50], Train Loss: 4.3073, Test Loss: 4.3160, Train Acc: 0.0543, Test Acc: 0.0456, F1 Score: 0.0210\n",
      "Epoch [6/50], Train Loss: 4.1494, Test Loss: 4.2488, Train Acc: 0.0768, Test Acc: 0.0381, F1 Score: 0.0239\n",
      "Epoch [7/50], Train Loss: 3.9881, Test Loss: 4.2399, Train Acc: 0.1005, Test Acc: 0.0468, F1 Score: 0.0251\n",
      "Epoch [8/50], Train Loss: 3.7775, Test Loss: 4.1389, Train Acc: 0.1269, Test Acc: 0.0639, F1 Score: 0.0415\n",
      "Epoch [9/50], Train Loss: 3.5606, Test Loss: 4.3313, Train Acc: 0.1638, Test Acc: 0.0552, F1 Score: 0.0324\n",
      "Epoch [10/50], Train Loss: 3.3342, Test Loss: 3.8344, Train Acc: 0.2061, Test Acc: 0.0981, F1 Score: 0.0749\n",
      "Epoch [11/50], Train Loss: 2.9907, Test Loss: 3.5666, Train Acc: 0.2885, Test Acc: 0.1605, F1 Score: 0.1460\n",
      "Epoch [12/50], Train Loss: 2.8111, Test Loss: 3.6913, Train Acc: 0.3161, Test Acc: 0.1083, F1 Score: 0.0998\n",
      "Epoch [13/50], Train Loss: 2.6601, Test Loss: 3.6237, Train Acc: 0.3644, Test Acc: 0.1344, F1 Score: 0.1228\n",
      "Epoch [14/50], Train Loss: 2.5248, Test Loss: 3.5253, Train Acc: 0.3860, Test Acc: 0.1428, F1 Score: 0.1364\n",
      "Epoch [15/50], Train Loss: 2.4063, Test Loss: 3.5371, Train Acc: 0.4127, Test Acc: 0.1560, F1 Score: 0.1481\n",
      "Epoch [16/50], Train Loss: 2.2482, Test Loss: 3.4441, Train Acc: 0.4412, Test Acc: 0.1638, F1 Score: 0.1601\n",
      "Epoch [17/50], Train Loss: 2.1277, Test Loss: 3.6188, Train Acc: 0.4724, Test Acc: 0.1548, F1 Score: 0.1445\n",
      "Epoch [18/50], Train Loss: 1.9601, Test Loss: 3.6328, Train Acc: 0.5135, Test Acc: 0.1488, F1 Score: 0.1435\n",
      "Epoch [19/50], Train Loss: 1.8237, Test Loss: 3.4626, Train Acc: 0.5537, Test Acc: 0.1902, F1 Score: 0.1902\n",
      "Epoch [20/50], Train Loss: 1.6792, Test Loss: 3.2546, Train Acc: 0.5933, Test Acc: 0.2259, F1 Score: 0.2200\n",
      "Epoch [21/50], Train Loss: 1.4665, Test Loss: 3.1620, Train Acc: 0.6542, Test Acc: 0.2421, F1 Score: 0.2342\n",
      "Epoch [22/50], Train Loss: 1.3814, Test Loss: 3.1918, Train Acc: 0.6821, Test Acc: 0.2373, F1 Score: 0.2349\n",
      "Epoch [23/50], Train Loss: 1.3174, Test Loss: 3.2258, Train Acc: 0.7109, Test Acc: 0.2394, F1 Score: 0.2350\n",
      "Epoch [24/50], Train Loss: 1.2583, Test Loss: 3.2747, Train Acc: 0.7166, Test Acc: 0.2301, F1 Score: 0.2348\n",
      "Epoch [25/50], Train Loss: 1.1968, Test Loss: 3.1279, Train Acc: 0.7463, Test Acc: 0.2529, F1 Score: 0.2531\n",
      "Epoch [26/50], Train Loss: 1.1209, Test Loss: 3.1203, Train Acc: 0.7609, Test Acc: 0.2667, F1 Score: 0.2579\n",
      "Epoch [27/50], Train Loss: 1.0781, Test Loss: 3.1382, Train Acc: 0.7759, Test Acc: 0.2631, F1 Score: 0.2611\n",
      "Epoch [28/50], Train Loss: 1.0105, Test Loss: 3.3526, Train Acc: 0.7894, Test Acc: 0.2307, F1 Score: 0.2411\n",
      "Epoch [29/50], Train Loss: 0.9585, Test Loss: 3.1018, Train Acc: 0.7975, Test Acc: 0.2640, F1 Score: 0.2617\n",
      "Epoch [30/50], Train Loss: 0.9064, Test Loss: 3.2729, Train Acc: 0.8188, Test Acc: 0.2346, F1 Score: 0.2361\n",
      "Epoch [31/50], Train Loss: 0.8227, Test Loss: 3.0616, Train Acc: 0.8398, Test Acc: 0.2835, F1 Score: 0.2843\n",
      "Epoch [32/50], Train Loss: 0.7933, Test Loss: 3.0596, Train Acc: 0.8548, Test Acc: 0.2766, F1 Score: 0.2765\n",
      "Epoch [33/50], Train Loss: 0.7810, Test Loss: 3.0956, Train Acc: 0.8524, Test Acc: 0.2757, F1 Score: 0.2681\n",
      "Epoch [34/50], Train Loss: 0.7566, Test Loss: 3.0299, Train Acc: 0.8623, Test Acc: 0.2850, F1 Score: 0.2792\n",
      "Epoch [35/50], Train Loss: 0.7248, Test Loss: 3.0555, Train Acc: 0.8770, Test Acc: 0.2877, F1 Score: 0.2871\n",
      "Epoch [36/50], Train Loss: 0.7151, Test Loss: 3.0392, Train Acc: 0.8704, Test Acc: 0.2895, F1 Score: 0.2878\n",
      "Epoch [37/50], Train Loss: 0.6750, Test Loss: 3.0633, Train Acc: 0.8833, Test Acc: 0.2850, F1 Score: 0.2832\n",
      "Epoch [38/50], Train Loss: 0.6596, Test Loss: 3.0556, Train Acc: 0.8788, Test Acc: 0.2841, F1 Score: 0.2827\n",
      "Epoch [39/50], Train Loss: 0.6423, Test Loss: 3.0399, Train Acc: 0.8887, Test Acc: 0.2898, F1 Score: 0.2895\n",
      "Epoch [40/50], Train Loss: 0.6355, Test Loss: 3.0322, Train Acc: 0.8908, Test Acc: 0.2904, F1 Score: 0.2888\n",
      "Epoch [41/50], Train Loss: 0.5829, Test Loss: 3.0365, Train Acc: 0.9055, Test Acc: 0.2946, F1 Score: 0.2928\n",
      "Epoch [42/50], Train Loss: 0.5766, Test Loss: 3.0405, Train Acc: 0.9094, Test Acc: 0.2910, F1 Score: 0.2909\n",
      "Epoch [43/50], Train Loss: 0.5789, Test Loss: 3.0396, Train Acc: 0.9103, Test Acc: 0.2958, F1 Score: 0.2932\n",
      "Epoch [44/50], Train Loss: 0.5714, Test Loss: 3.0280, Train Acc: 0.9136, Test Acc: 0.2913, F1 Score: 0.2911\n",
      "Epoch [45/50], Train Loss: 0.5705, Test Loss: 3.0406, Train Acc: 0.9157, Test Acc: 0.2991, F1 Score: 0.2982\n",
      "Epoch [46/50], Train Loss: 0.5423, Test Loss: 3.0575, Train Acc: 0.9175, Test Acc: 0.2937, F1 Score: 0.2934\n",
      "Epoch [47/50], Train Loss: 0.5422, Test Loss: 3.0322, Train Acc: 0.9145, Test Acc: 0.3012, F1 Score: 0.2992\n",
      "Epoch [48/50], Train Loss: 0.5409, Test Loss: 3.0453, Train Acc: 0.9187, Test Acc: 0.2937, F1 Score: 0.2926\n",
      "Epoch [49/50], Train Loss: 0.5241, Test Loss: 3.0376, Train Acc: 0.9226, Test Acc: 0.3021, F1 Score: 0.3003\n",
      "Epoch [50/50], Train Loss: 0.5116, Test Loss: 3.0512, Train Acc: 0.9271, Test Acc: 0.2946, F1 Score: 0.2931\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images'\n",
    "\n",
    "# Paths to the text files\n",
    "train_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_variant_train.txt'\n",
    "test_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_variant_test.txt'\n",
    "val_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_variant_trainval.txt'\n",
    "\n",
    "# Define label mapping with updated aircraft models\n",
    "label_mapping = {\n",
    "    \"707-320\": 0,\n",
    "    \"727-200\": 1,\n",
    "    \"737-200\": 2,\n",
    "    \"737-300\": 3,\n",
    "    \"737-400\": 4,\n",
    "    \"737-500\": 5,\n",
    "    \"737-600\": 6,\n",
    "    \"737-700\": 7,\n",
    "    \"737-800\": 8,\n",
    "    \"737-900\": 9,\n",
    "    \"747-100\": 10,\n",
    "    \"747-200\": 11,\n",
    "    \"747-300\": 12,\n",
    "    \"747-400\": 13,\n",
    "    \"757-200\": 14,\n",
    "    \"757-300\": 15,\n",
    "    \"767-200\": 16,\n",
    "    \"767-300\": 17,\n",
    "    \"767-400\": 18,\n",
    "    \"777-200\": 19,\n",
    "    \"777-300\": 20,\n",
    "    \"A300B4\": 21,\n",
    "    \"A310\": 22,\n",
    "    \"A318\": 23,\n",
    "    \"A319\": 24,\n",
    "    \"A320\": 25,\n",
    "    \"A321\": 26,\n",
    "    \"A330-200\": 27,\n",
    "    \"A330-300\": 28,\n",
    "    \"A340-200\": 29,\n",
    "    \"A340-300\": 30,\n",
    "    \"A340-500\": 31,\n",
    "    \"A340-600\": 32,\n",
    "    \"A380\": 33,\n",
    "    \"ATR-42\": 34,\n",
    "    \"ATR-72\": 35,\n",
    "    \"An-12\": 36,\n",
    "    \"BAE 146-200\": 37,\n",
    "    \"BAE 146-300\": 38,\n",
    "    \"BAE-125\": 39,\n",
    "    \"Beechcraft 1900\": 40,\n",
    "    \"Boeing 717\": 41,\n",
    "    \"C-130\": 42,\n",
    "    \"C-47\": 43,\n",
    "    \"CRJ-200\": 44,\n",
    "    \"CRJ-700\": 45,\n",
    "    \"CRJ-900\": 46,\n",
    "    \"Cessna 172\": 47,\n",
    "    \"Cessna 208\": 48,\n",
    "    \"Cessna 525\": 49,\n",
    "    \"Cessna 560\": 50,\n",
    "    \"Challenger 600\": 51,\n",
    "    \"DC-10\": 52,\n",
    "    \"DC-3\": 53,\n",
    "    \"DC-6\": 54,\n",
    "    \"DC-8\": 55,\n",
    "    \"DC-9-30\": 56,\n",
    "    \"DH-82\": 57,\n",
    "    \"DHC-1\": 58,\n",
    "    \"DHC-6\": 59,\n",
    "    \"DHC-8-100\": 60,\n",
    "    \"DHC-8-300\": 61,\n",
    "    \"DR-400\": 62,\n",
    "    \"Dornier 328\": 63,\n",
    "    \"E-170\": 64,\n",
    "    \"E-190\": 65,\n",
    "    \"E-195\": 66,\n",
    "    \"EMB-120\": 67,\n",
    "    \"ERJ 135\": 68,\n",
    "    \"ERJ 145\": 69,\n",
    "    \"Embraer Legacy 600\": 70,\n",
    "    \"Eurofighter Typhoon\": 71,\n",
    "    \"F-16A/B\": 72,\n",
    "    \"F/A-18\": 73,\n",
    "    \"Falcon 2000\": 74,\n",
    "    \"Falcon 900\": 75,\n",
    "    \"Fokker 100\": 76,\n",
    "    \"Fokker 50\": 77,\n",
    "    \"Fokker 70\": 78,\n",
    "    \"Global Express\": 79,\n",
    "    \"Gulfstream IV\": 80,\n",
    "    \"Gulfstream V\": 81,\n",
    "    \"Hawk T1\": 82,\n",
    "    \"Il-76\": 83,\n",
    "    \"L-1011\": 84,\n",
    "    \"MD-11\": 85,\n",
    "    \"MD-80\": 86,\n",
    "    \"MD-87\": 87,\n",
    "    \"MD-90\": 88,\n",
    "    \"Metroliner\": 89,\n",
    "    \"Model B200\": 90,\n",
    "    \"PA-28\": 91,\n",
    "    \"SR-20\": 92,\n",
    "    \"Saab 2000\": 93,\n",
    "    \"Saab 340\": 94,\n",
    "    \"Spitfire\": 95,\n",
    "    \"Tornado\": 96,\n",
    "    \"Tu-134\": 97,\n",
    "    \"Tu-154\": 98,\n",
    "    \"Yak-42\": 99\n",
    "}\n",
    "\n",
    "# Use all classes in label mapping\n",
    "selected_labels = label_mapping\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(5),\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5], [0.5])\n",
    "])\n",
    "\n",
    "def filter_data(label_file, selected_labels):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue\n",
    "            filename, label = parts\n",
    "            if label in selected_labels:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        image_tensor = transform(image)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(selected_labels[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Process training, validation, and test data\n",
    "train_image_tensor, train_label_tensor, train_labels = filter_data(train_label_file, selected_labels)\n",
    "val_image_tensor, val_label_tensor, val_labels = filter_data(val_label_file, selected_labels)\n",
    "test_image_tensor, test_label_tensor, test_labels = filter_data(test_label_file, selected_labels)\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_image_tensor, train_label_tensor)\n",
    "val_dataset = TensorDataset(val_image_tensor, val_label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n",
    "\n",
    "# DataLoaders\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define a full CNN model\n",
    "class FullCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(FullCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.fc3 = nn.Linear(256, len(selected_labels))\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the model\n",
    "model = FullCNN()\n",
    "\n",
    "# Loss and optimiser\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    scheduler.step()\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "    \n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b291ae79-b1d8-49c9-add8-f81d04da2c68",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Plot Loss\u001b[39;00m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 5\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, train_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTraining Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(epochs, test_losses, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTest Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEpochs\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_losses' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7f7fa-bad2-44bc-9af7-7016e0ae5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b40c5-b476-45d1-9f84-b5aaebe3852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Improved Model F1 Scores\n",
    "plt.plot(epochs, f1_scores, linestyle='-', label=\"Improved Model\")\n",
    "\n",
    "# Plot the F1 values from the Baseline Model, this is hard-coded in\n",
    "plt.plot(\n",
    "    range(1, 16), \n",
    "    [0.2885368466152528, 0.2885368466152528, 0.2885368466152528, 0.35800560282056604, \n",
    "     0.2885368466152528, 0.2885368466152528, 0.28990515812498047, 0.2885368466152528, \n",
    "     0.36161119202439296, 0.2913618247972723, 0.3724076408386039, 0.3755616336834515, \n",
    "     0.29490095228198265, 0.29349818381047593, 0.32165556746495966], linestyle='-', label=\"Simple Model\"\n",
    ")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e179b4-935e-4308-b58d-db5d33112b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [0.01] * 15\n",
    "\n",
    "# Plot improved model (all 50 epochs)\n",
    "plt.plot(range(1, num_epochs + 1), learning_rates, marker='o', linestyle='-', label=\"Improved Model\")\n",
    "# Plot baseline model (up to epoch 15)\n",
    "plt.plot(range(1, 16), baseline, linestyle='--', color='red', label=\"Baseline Model\")\n",
    "plt.title(\"Learning Rate vs. Epoch Number\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
