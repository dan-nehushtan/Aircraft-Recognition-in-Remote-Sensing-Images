{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fda6d11a-60b6-401e-a4ca-e805feb61027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 2367\n",
      "Number of validation images: 4732\n",
      "Number of test images: 2368\n",
      "Training dataset class distribution:\n",
      "Boeing: 733\n",
      "Airbus: 434\n",
      "ATR: 66\n",
      "Antonov: 34\n",
      "Beechcraft: 67\n",
      "Canadair: 134\n",
      "Cessna: 133\n",
      "Robin: 33\n",
      "Dornier: 34\n",
      "Embraer: 233\n",
      "Eurofighter: 33\n",
      "Fokker: 100\n",
      "Ilyushin: 33\n",
      "Fairchild: 33\n",
      "Piper: 33\n",
      "Saab: 67\n",
      "Supermarine: 33\n",
      "Panavia: 34\n",
      "Tupolev: 66\n",
      "Yakovlev: 34\n",
      "Validation dataset class distribution:\n",
      "Boeing: 1466\n",
      "Airbus: 867\n",
      "ATR: 133\n",
      "Antonov: 67\n",
      "Beechcraft: 134\n",
      "Canadair: 267\n",
      "Cessna: 266\n",
      "Robin: 66\n",
      "Dornier: 67\n",
      "Embraer: 467\n",
      "Eurofighter: 66\n",
      "Fokker: 200\n",
      "Ilyushin: 66\n",
      "Fairchild: 66\n",
      "Piper: 67\n",
      "Saab: 134\n",
      "Supermarine: 66\n",
      "Panavia: 67\n",
      "Tupolev: 133\n",
      "Yakovlev: 67\n",
      "Test dataset class distribution:\n",
      "Boeing: 734\n",
      "Airbus: 433\n",
      "ATR: 67\n",
      "Antonov: 33\n",
      "Beechcraft: 66\n",
      "Canadair: 133\n",
      "Cessna: 134\n",
      "Robin: 34\n",
      "Dornier: 33\n",
      "Embraer: 233\n",
      "Eurofighter: 34\n",
      "Fokker: 100\n",
      "Ilyushin: 34\n",
      "Fairchild: 34\n",
      "Piper: 33\n",
      "Saab: 66\n",
      "Supermarine: 34\n",
      "Panavia: 33\n",
      "Tupolev: 67\n",
      "Yakovlev: 33\n",
      "DataLoaders for all labels created successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'selected_labels' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 169\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[1;32m    168\u001b[0m \u001b[38;5;66;03m# Instantiate the simplified model\u001b[39;00m\n\u001b[0;32m--> 169\u001b[0m model \u001b[38;5;241m=\u001b[39m SimpleCNN()\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# Define loss function and optimizer\u001b[39;00m\n\u001b[1;32m    172\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n",
      "Cell \u001b[0;32mIn[7], line 159\u001b[0m, in \u001b[0;36mSimpleCNN.__init__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBatchNorm2d(\u001b[38;5;241m16\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mMaxPool2d(\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1 \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mLinear(\u001b[38;5;241m16\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m64\u001b[39m, \u001b[38;5;28mlen\u001b[39m(selected_labels))\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mReLU()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'selected_labels' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images'\n",
    "\n",
    "# Paths to the text files\n",
    "train_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_train.txt'\n",
    "test_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_test.txt'\n",
    "val_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_trainval.txt'\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"Boeing\": 0,\n",
    "    \"Airbus\": 1,\n",
    "    \"ATR\": 2,\n",
    "    \"Antonov\": 3,\n",
    "    \"BritishAerospace\": 4,\n",
    "    \"Beechcraft\": 5,\n",
    "    \"LockheedCorporation\": 6,\n",
    "    \"DouglasAircraftCompany\": 7,\n",
    "    \"Canadair\": 8,\n",
    "    \"Cessna\": 9,\n",
    "    \"McDonnellDouglas\": 10,\n",
    "    \"deHavilland\": 11,\n",
    "    \"Robin\": 12,\n",
    "    \"Dornier\": 13,\n",
    "    \"Embraer\": 14,\n",
    "    \"Eurofighter\": 15,\n",
    "    \"LockheedMartin\": 16,\n",
    "    \"DassaultAviation\": 17,\n",
    "    \"Fokker\": 18,\n",
    "    \"BombardierAerospace\": 19,\n",
    "    \"GulfstreamAerospace\": 20,\n",
    "    \"Ilyushin\": 21,\n",
    "    \"Fairchild\": 22,\n",
    "    \"Piper\": 23,\n",
    "    \"CirrusAircraft\": 24,\n",
    "    \"Saab\": 25,\n",
    "    \"Supermarine\": 26,\n",
    "    \"Panavia\": 27,\n",
    "    \"Tupolev\": 28,\n",
    "    \"Yakovlev\": 29\n",
    "}\n",
    "\n",
    "def filter_data(label_file, label_mapping):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            filename, label = parts\n",
    "            if label in label_mapping:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        image_tensor = transform(image)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(label_mapping[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomRotation(5),    # Random rotation up to 15 degrees\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize\n",
    "])\n",
    "\n",
    "# Process training, validation, and test data for all labels\n",
    "train_image_tensor, train_label_tensor, train_labels = filter_data(train_label_file, label_mapping)\n",
    "val_image_tensor, val_label_tensor, val_labels = filter_data(val_label_file, label_mapping)\n",
    "test_image_tensor, test_label_tensor, test_labels = filter_data(test_label_file, label_mapping)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training images: {len(train_image_tensor)}\")\n",
    "print(f\"Number of validation images: {len(val_image_tensor)}\")\n",
    "print(f\"Number of test images: {len(test_image_tensor)}\")\n",
    "\n",
    "# Print counts for each class in training, validation, and test datasets\n",
    "train_counts = Counter(train_labels)\n",
    "val_counts = Counter(val_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "print(\"Training dataset class distribution:\")\n",
    "for label, count in train_counts.items():\n",
    "    print(f\"{list(label_mapping.keys())[list(label_mapping.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Validation dataset class distribution:\")\n",
    "for label, count in val_counts.items():\n",
    "    print(f\"{list(label_mapping.keys())[list(label_mapping.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Test dataset class distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"{list(label_mapping.keys())[list(label_mapping.values()).index(label)]}: {count}\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_image_tensor, train_label_tensor)\n",
    "val_dataset = TensorDataset(val_image_tensor, val_label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "batch_size = 32  # Increased batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"DataLoaders for all labels created successfully.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1696be7a-b9a2-4200-a501-89981453c6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/40], Train Loss: 7.4205, Test Loss: 2.3751, Train Acc: 0.2290, Test Acc: 0.2673, F1 Score: 0.2038, Learning Rate: 0.01\n",
      "Epoch [2/40], Train Loss: 2.0793, Test Loss: 2.5723, Train Acc: 0.3507, Test Acc: 0.3171, F1 Score: 0.1800, Learning Rate: 0.01\n",
      "Epoch [3/40], Train Loss: 1.7512, Test Loss: 2.4576, Train Acc: 0.4292, Test Acc: 0.3252, F1 Score: 0.2278, Learning Rate: 0.01\n",
      "Epoch [4/40], Train Loss: 1.6193, Test Loss: 2.0699, Train Acc: 0.4854, Test Acc: 0.3353, F1 Score: 0.2820, Learning Rate: 0.01\n",
      "Epoch [5/40], Train Loss: 1.4833, Test Loss: 2.0741, Train Acc: 0.5188, Test Acc: 0.3480, F1 Score: 0.3166, Learning Rate: 0.01\n",
      "Epoch [6/40], Train Loss: 1.1795, Test Loss: 2.1893, Train Acc: 0.6341, Test Acc: 0.3552, F1 Score: 0.2885, Learning Rate: 0.01\n",
      "Epoch [7/40], Train Loss: 1.0588, Test Loss: 2.4803, Train Acc: 0.6582, Test Acc: 0.2851, F1 Score: 0.2398, Learning Rate: 0.005\n",
      "Epoch [8/40], Train Loss: 0.5992, Test Loss: 2.0636, Train Acc: 0.8475, Test Acc: 0.3522, F1 Score: 0.3408, Learning Rate: 0.005\n",
      "Epoch [9/40], Train Loss: 0.4618, Test Loss: 1.9969, Train Acc: 0.8952, Test Acc: 0.3767, F1 Score: 0.3656, Learning Rate: 0.005\n",
      "Epoch [10/40], Train Loss: 0.3630, Test Loss: 2.2020, Train Acc: 0.9332, Test Acc: 0.3505, F1 Score: 0.3349, Learning Rate: 0.005\n",
      "Epoch [11/40], Train Loss: 0.2826, Test Loss: 2.0702, Train Acc: 0.9565, Test Acc: 0.3974, F1 Score: 0.3804, Learning Rate: 0.005\n",
      "Epoch [12/40], Train Loss: 0.2134, Test Loss: 2.0323, Train Acc: 0.9797, Test Acc: 0.4084, F1 Score: 0.3927, Learning Rate: 0.005\n",
      "Epoch [13/40], Train Loss: 0.1707, Test Loss: 2.0932, Train Acc: 0.9882, Test Acc: 0.4067, F1 Score: 0.3934, Learning Rate: 0.005\n",
      "Epoch [14/40], Train Loss: 0.1266, Test Loss: 2.1339, Train Acc: 0.9949, Test Acc: 0.4041, F1 Score: 0.3954, Learning Rate: 0.0025\n",
      "Epoch [15/40], Train Loss: 0.0986, Test Loss: 2.1372, Train Acc: 0.9975, Test Acc: 0.4067, F1 Score: 0.3908, Learning Rate: 0.0025\n",
      "Epoch [16/40], Train Loss: 0.0897, Test Loss: 2.1758, Train Acc: 0.9987, Test Acc: 0.4084, F1 Score: 0.3893, Learning Rate: 0.0025\n",
      "Epoch [17/40], Train Loss: 0.0825, Test Loss: 2.1274, Train Acc: 0.9987, Test Acc: 0.4168, F1 Score: 0.4033, Learning Rate: 0.0025\n",
      "Epoch [18/40], Train Loss: 0.0757, Test Loss: 2.1738, Train Acc: 0.9987, Test Acc: 0.4058, F1 Score: 0.3876, Learning Rate: 0.0025\n",
      "Epoch [19/40], Train Loss: 0.0699, Test Loss: 2.1718, Train Acc: 0.9992, Test Acc: 0.4084, F1 Score: 0.3919, Learning Rate: 0.0025\n",
      "Epoch [20/40], Train Loss: 0.0642, Test Loss: 2.2118, Train Acc: 0.9996, Test Acc: 0.4075, F1 Score: 0.3884, Learning Rate: 0.0025\n",
      "Epoch [21/40], Train Loss: 0.0600, Test Loss: 2.2074, Train Acc: 0.9996, Test Acc: 0.4126, F1 Score: 0.3972, Learning Rate: 0.00125\n",
      "Epoch [22/40], Train Loss: 0.0553, Test Loss: 2.2033, Train Acc: 0.9996, Test Acc: 0.4092, F1 Score: 0.3902, Learning Rate: 0.00125\n",
      "Epoch [23/40], Train Loss: 0.0530, Test Loss: 2.2068, Train Acc: 0.9996, Test Acc: 0.4101, F1 Score: 0.3939, Learning Rate: 0.00125\n",
      "Epoch [24/40], Train Loss: 0.0518, Test Loss: 2.2320, Train Acc: 0.9996, Test Acc: 0.4113, F1 Score: 0.3958, Learning Rate: 0.00125\n",
      "Epoch [25/40], Train Loss: 0.0500, Test Loss: 2.2530, Train Acc: 0.9996, Test Acc: 0.4122, F1 Score: 0.3928, Learning Rate: 0.00125\n",
      "Epoch [26/40], Train Loss: 0.0497, Test Loss: 2.2463, Train Acc: 0.9996, Test Acc: 0.4109, F1 Score: 0.3933, Learning Rate: 0.00125\n",
      "Epoch [27/40], Train Loss: 0.0475, Test Loss: 2.2456, Train Acc: 0.9996, Test Acc: 0.4109, F1 Score: 0.3929, Learning Rate: 0.00125\n",
      "Epoch [28/40], Train Loss: 0.0460, Test Loss: 2.2275, Train Acc: 0.9996, Test Acc: 0.4117, F1 Score: 0.3951, Learning Rate: 0.000625\n",
      "Epoch [29/40], Train Loss: 0.0439, Test Loss: 2.2399, Train Acc: 0.9996, Test Acc: 0.4101, F1 Score: 0.3937, Learning Rate: 0.000625\n",
      "Epoch [30/40], Train Loss: 0.0435, Test Loss: 2.2382, Train Acc: 0.9996, Test Acc: 0.4117, F1 Score: 0.3952, Learning Rate: 0.000625\n",
      "Epoch [31/40], Train Loss: 0.0431, Test Loss: 2.2575, Train Acc: 0.9996, Test Acc: 0.4147, F1 Score: 0.3989, Learning Rate: 0.000625\n",
      "Epoch [32/40], Train Loss: 0.0431, Test Loss: 2.2317, Train Acc: 0.9996, Test Acc: 0.4096, F1 Score: 0.3930, Learning Rate: 0.000625\n",
      "Epoch [33/40], Train Loss: 0.0422, Test Loss: 2.2425, Train Acc: 0.9996, Test Acc: 0.4109, F1 Score: 0.3953, Learning Rate: 0.000625\n",
      "Epoch [34/40], Train Loss: 0.0414, Test Loss: 2.2635, Train Acc: 0.9996, Test Acc: 0.4139, F1 Score: 0.3980, Learning Rate: 0.000625\n",
      "Epoch [35/40], Train Loss: 0.0411, Test Loss: 2.2471, Train Acc: 0.9996, Test Acc: 0.4126, F1 Score: 0.3954, Learning Rate: 0.0003125\n",
      "Epoch [36/40], Train Loss: 0.0408, Test Loss: 2.2537, Train Acc: 0.9996, Test Acc: 0.4105, F1 Score: 0.3941, Learning Rate: 0.0003125\n",
      "Epoch [37/40], Train Loss: 0.0402, Test Loss: 2.2622, Train Acc: 0.9996, Test Acc: 0.4092, F1 Score: 0.3928, Learning Rate: 0.0003125\n",
      "Epoch [38/40], Train Loss: 0.0395, Test Loss: 2.2836, Train Acc: 0.9996, Test Acc: 0.4101, F1 Score: 0.3930, Learning Rate: 0.0003125\n",
      "Epoch [39/40], Train Loss: 0.0394, Test Loss: 2.2654, Train Acc: 0.9996, Test Acc: 0.4122, F1 Score: 0.3960, Learning Rate: 0.0003125\n",
      "Epoch [40/40], Train Loss: 0.0391, Test Loss: 2.2847, Train Acc: 0.9996, Test Acc: 0.4117, F1 Score: 0.3948, Learning Rate: 0.0003125\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "# Define a simplified CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 16 * 16, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, len(selected_labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(16 * 64 * 64, len(label_mapping))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = x.view(-1, 16 * 64 * 64)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the simplified model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Fixed learning rate\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.5)  # Decays learning rate by 0.1 every 10 epochs\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "f1_scores = []\n",
    "learning_rates = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 40  # Increased number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step()\n",
    "    learning_rates.append(scheduler.get_last_lr()[0])\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, F1 Score: {f1:.4f}, Learning Rate: {scheduler.get_last_lr()[0]}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Create Epochs for plotting results\n",
    "epochs = range(1, num_epochs + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b291ae79-b1d8-49c9-add8-f81d04da2c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8a7f7fa-bad2-44bc-9af7-7016e0ae5348",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f1b40c5-b476-45d1-9f84-b5aaebe3852f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "# Improved Model F1 Scores\n",
    "plt.plot(epochs, f1_scores, linestyle='-', label=\"Improved Model\")\n",
    "\n",
    "# Plot the F1 values from the Baseline Model, this is hard-coded in\n",
    "plt.plot(\n",
    "    range(1, 16), \n",
    "    [0.2885368466152528, 0.2885368466152528, 0.2885368466152528, 0.35800560282056604, \n",
    "     0.2885368466152528, 0.2885368466152528, 0.28990515812498047, 0.2885368466152528, \n",
    "     0.36161119202439296, 0.2913618247972723, 0.3724076408386039, 0.3755616336834515, \n",
    "     0.29490095228198265, 0.29349818381047593, 0.32165556746495966], linestyle='-', label=\"Simple Model\"\n",
    ")\n",
    "\n",
    "# Formatting the plot\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e179b4-935e-4308-b58d-db5d33112b75",
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = [0.01] * 15\n",
    "\n",
    "# Plot improved model (all 50 epochs)\n",
    "plt.plot(range(1, num_epochs + 1), learning_rates, marker='o', linestyle='-', label=\"Improved Model\")\n",
    "# Plot baseline model (up to epoch 15)\n",
    "plt.plot(range(1, 16), baseline, linestyle='--', color='red', label=\"Baseline Model\")\n",
    "plt.title(\"Learning Rate vs. Epoch Number\")\n",
    "plt.xlabel(\"Epoch Number\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
