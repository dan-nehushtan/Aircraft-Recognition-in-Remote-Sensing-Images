{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bfe0a73-9a26-4730-b48c-455b1cf87148",
   "metadata": {},
   "outputs": [],
   "source": [
    "######\n",
    "\n",
    "#Firstly create a tensor which contains the iamges and their labels\n",
    "\n",
    "######"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "79517f0e-4b25-416d-8552-06f269e0b440",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label Mapping: {'Boeing 707': 0, 'Boeing 727': 1, 'Boeing 737': 2, 'Boeing 747': 3, 'Boeing 757': 4, 'Boeing 767': 5, 'Boeing 777': 6, 'A300': 7, 'A310': 8, 'A320': 9, 'A330': 10, 'A340': 11, 'A380': 12, 'ATR-42': 13, 'ATR-72': 14, 'An-12': 15, 'BAE 146': 16, 'BAE-125': 17, 'Beechcraft 1900': 18, 'Boeing 717': 19, 'C-130': 20, 'C-47': 21, 'CRJ-200': 22, 'CRJ-700': 23, 'Cessna 172': 24, 'Cessna 208': 25, 'Cessna Citation': 26, 'Challenger 600': 27, 'DC-10': 28, 'DC-3': 29, 'DC-6': 30, 'DC-8': 31, 'DC-9': 32, 'DH-82': 33, 'DHC-1': 34, 'DHC-6': 35, 'Dash 8': 36, 'DR-400': 37, 'Dornier 328': 38, 'Embraer E-Jet': 39, 'EMB-120': 40, 'Embraer ERJ 145': 41, 'Embraer Legacy 600': 42, 'Eurofighter Typhoon': 43, 'F-16': 44, 'F/A-18': 45, 'Falcon 2000': 46, 'Falcon 900': 47, 'Fokker 100': 48, 'Fokker 50': 49, 'Fokker 70': 50, 'Global Express': 51, 'Gulfstream': 52, 'Hawk T1': 53, 'Il-76': 54, 'L-1011': 55, 'MD-11': 56, 'MD-80': 57, 'MD-90': 58, 'Metroliner': 59, 'King Air': 60, 'PA-28': 61, 'SR-20': 62, 'Saab 2000': 63, 'Saab 340': 64, 'Spitfire': 65, 'Tornado': 66, 'Tu-134': 67, 'Tu-154': 68, 'Yak-42': 69}\n",
      "Image tensor shape: torch.Size([3334, 3, 128, 128])\n",
      "Label tensor shape: torch.Size([3334])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images'\n",
    "\n",
    "# Path to the text file\n",
    "label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_family_train.txt'\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),  # Resize to 128x128 (can be adjusted)\n",
    "    transforms.ToTensor()          # Convert to tensor\n",
    "])\n",
    "\n",
    "# Parse the label file and create a mapping from unique labels to integers starting at 0\n",
    "label_mapping = {}\n",
    "image_data = []\n",
    "labels = []\n",
    "\n",
    "with open(label_file, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)\n",
    "        if len(parts) != 2:\n",
    "            continue  # Skip malformed lines\n",
    "        filename, label = parts\n",
    "        \n",
    "        # Assign a new integer label if not already mapped\n",
    "        if label not in label_mapping:\n",
    "            label_mapping[label] = len(label_mapping)\n",
    "        \n",
    "        # Process the image\n",
    "        image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')\n",
    "                image_tensor = transform(image)\n",
    "                image_data.append(image_tensor)\n",
    "                labels.append(label_mapping[label])\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "# Convert lists to PyTorch tensors\n",
    "image_tensor = torch.stack(image_data)  # Create a tensor from image data\n",
    "label_tensor = torch.tensor(labels)     # Convert labels to tensor\n",
    "\n",
    "# Print label mapping and tensor dimensions for verification\n",
    "print(\"Label Mapping:\", label_mapping)\n",
    "print(\"Image tensor shape:\", image_tensor.shape)\n",
    "print(\"Label tensor shape:\", label_tensor.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6ffb3b5e-c08d-4300-83aa-9b7fc321759a",
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "\n",
    "#Create the testing data set\n",
    "\n",
    "#########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5406854b-291f-4782-8ab2-91958b3da1f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Image Tensor Shape: torch.Size([3333, 3, 128, 128])\n",
      "Test Label Tensor Shape: torch.Size([3333])\n"
     ]
    }
   ],
   "source": [
    "# Path to the testing data file\n",
    "testing_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_family_test.txt'\n",
    "\n",
    "test_image_data = []\n",
    "test_labels = []\n",
    "\n",
    "with open(testing_file, 'r') as f:\n",
    "    for line in f:\n",
    "        parts = line.strip().split(maxsplit=1)  # Split into filename and label\n",
    "        if len(parts) != 2:\n",
    "            continue  # Skip malformed lines\n",
    "        filename, label = parts\n",
    "\n",
    "        # Check if the label exists in the training label mapping\n",
    "        if label not in label_mapping:\n",
    "            print(f\"Warning: Label '{label}' in testing data not found in training labels.\")\n",
    "            continue  # Skip labels not seen during training\n",
    "\n",
    "        # Construct image path and process if exists\n",
    "        image_path = os.path.join(image_folder, filename + \".jpg\")  # Assuming .jpg extension\n",
    "        if os.path.exists(image_path):\n",
    "            try:\n",
    "                image = Image.open(image_path).convert('RGB')  # Open image\n",
    "                image_tensor = transform(image)               # Apply transformations\n",
    "                test_image_data.append(image_tensor)\n",
    "                test_labels.append(label_mapping[label])      # Map the label to its integer value\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing file {image_path}: {e}\")\n",
    "        else:\n",
    "            print(f\"File not found: {image_path}\")\n",
    "\n",
    "# Ensure data synchronization\n",
    "assert len(test_image_data) == len(test_labels), \"Mismatch in number of test images and labels!\"\n",
    "\n",
    "# Convert to tensors\n",
    "test_image_tensor = torch.stack(test_image_data)  # Stack images into a tensor\n",
    "test_label_tensor = torch.tensor(test_labels)     # Convert labels to a tensor\n",
    "\n",
    "# Verify shapes\n",
    "print(\"Test Image Tensor Shape:\", test_image_tensor.shape)\n",
    "print(\"Test Label Tensor Shape:\", test_label_tensor.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ab775-c4f2-4909-bbb5-5564a5b845de",
   "metadata": {},
   "outputs": [],
   "source": [
    "########\n",
    "\n",
    "#Creating the CNN\n",
    "\n",
    "########\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "baedeee4-0a08-4ec6-a022-0d8dc748b25d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the CNN...\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (32x262144 and 65536x128)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[57], line 65\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     64\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()  \u001b[38;5;66;03m# Zero the parameter gradients\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m model(images)\n\u001b[1;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     67\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()  \u001b[38;5;66;03m# Backpropagation\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[57], line 29\u001b[0m, in \u001b[0;36mCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     27\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool(torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2(x)))\n\u001b[1;32m     28\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten the tensor\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x))\n\u001b[1;32m     30\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x)\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (32x262144 and 65536x128)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define the CNN architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)  # Input channels = 3 (RGB), Output channels = 32\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Max pooling\n",
    "        self.fc1_input_size = self._get_flatten_size()\n",
    "        self.fc1 = nn.Linear(self.fc1_input_size, 128)  # Adjusted input size\n",
    "        self.fc2 = nn.Linear(128, num_classes)\n",
    "\n",
    "    def _get_flatten_size(self):\n",
    "        # Use a dummy tensor to calculate the flattened size\n",
    "        with torch.no_grad():\n",
    "            dummy_input = torch.zeros(1, 3, 128, 128)  # Example input tensor\n",
    "            x = self.pool(torch.relu(self.conv1(dummy_input)))\n",
    "            x = self.pool(torch.relu(self.conv2(x)))\n",
    "            return x.view(1, -1).size(1)  # Flatten and get the size\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = self.pool(torch.relu(self.conv2(x)))\n",
    "        x = x.view(x.size(0), -1)  # Flatten the tensor\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Parameters\n",
    "num_classes = len(label_mapping)  # Number of unique labels\n",
    "batch_size = 32\n",
    "learning_rate = 0.001\n",
    "num_epochs = 10\n",
    "\n",
    "# Ensure label tensor shape is correct\n",
    "label_tensor = label_tensor.squeeze()  # Remove extra dimensions if any\n",
    "\n",
    "image_tensor = torch.stack(image_data)  # Ensure proper tensor stacking for images\n",
    "label_tensor = torch.tensor(labels, dtype=torch.long)  # Ensure labels are integer tensors\n",
    "\n",
    "# Create TensorDataset\n",
    "train_dataset = TensorDataset(image_tensor, label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "model = CNN(num_classes=num_classes)\n",
    "criterion = nn.CrossEntropyLoss()  # Use cross-entropy loss for classification\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training the CNN\n",
    "print(\"Training the CNN...\")\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set the model to training mode\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()  # Zero the parameter gradients\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Optimize weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
    "\n",
    "# Evaluate the CNN on the test set\n",
    "print(\"Evaluating the CNN...\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "accuracy = 100 * correct / total\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762749d2-d93d-40eb-8e7c-7c09a6fc9017",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### \n",
    "\n",
    "#F1 Calculation Score\n",
    "\n",
    "#####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ff0e3-1074-4bda-b50a-0d61197d63cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Evaluate the CNN on the test set and calculate accuracy and F1-score\n",
    "print(\"Evaluating the CNN...\")\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)  # Get the class with the highest score\n",
    "        all_labels.extend(labels.numpy())      # Store true labels\n",
    "        all_predictions.extend(predicted.numpy())  # Store predictions\n",
    "\n",
    "# Calculate Accuracy\n",
    "correct = sum(1 for true, pred in zip(all_labels, all_predictions) if true == pred)\n",
    "total = len(all_labels)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "# Calculate F1-score\n",
    "f1 = f1_score(all_labels, all_predictions, average='weighted')\n",
    "\n",
    "print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "print(f\"F1 Score: {f1:.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
