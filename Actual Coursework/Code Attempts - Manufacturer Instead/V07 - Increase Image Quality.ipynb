{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8be393e-c7d4-464b-8ca1-168979018886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1667\n",
      "Number of validation images: 3333\n",
      "Number of test images: 1667\n",
      "Training dataset class distribution:\n",
      "Boeing: 733\n",
      "Airbus: 434\n",
      "Canadair: 134\n",
      "Cessna: 133\n",
      "Embraer: 233\n",
      "Validation dataset class distribution:\n",
      "Boeing: 1466\n",
      "Airbus: 867\n",
      "Canadair: 267\n",
      "Cessna: 266\n",
      "Embraer: 467\n",
      "Test dataset class distribution:\n",
      "Boeing: 734\n",
      "Airbus: 433\n",
      "Canadair: 133\n",
      "Cessna: 134\n",
      "Embraer: 233\n",
      "DataLoaders for selected labels created successfully.\n",
      "Epoch [1/30], Train Loss: 1.6433, Test Loss: 1.6084, Train Acc: 0.3653, Test Acc: 0.4403, F1 Score: 0.2692\n",
      "Epoch [2/30], Train Loss: 1.4127, Test Loss: 1.4714, Train Acc: 0.4025, Test Acc: 0.4403, F1 Score: 0.2692\n",
      "Epoch [3/30], Train Loss: 1.3876, Test Loss: 1.3661, Train Acc: 0.4151, Test Acc: 0.4403, F1 Score: 0.2692\n",
      "Epoch [4/30], Train Loss: 1.3443, Test Loss: 1.3443, Train Acc: 0.4313, Test Acc: 0.4223, F1 Score: 0.3222\n",
      "Epoch [5/30], Train Loss: 1.3102, Test Loss: 2.3003, Train Acc: 0.4283, Test Acc: 0.2597, F1 Score: 0.1072\n",
      "Epoch [6/30], Train Loss: 1.3951, Test Loss: 1.2586, Train Acc: 0.4325, Test Acc: 0.4463, F1 Score: 0.2881\n",
      "Epoch [7/30], Train Loss: 1.3508, Test Loss: 1.3869, Train Acc: 0.4223, Test Acc: 0.4391, F1 Score: 0.2744\n",
      "Epoch [8/30], Train Loss: 1.3160, Test Loss: 1.7254, Train Acc: 0.4283, Test Acc: 0.4421, F1 Score: 0.2732\n",
      "Epoch [9/30], Train Loss: 1.3170, Test Loss: 1.3138, Train Acc: 0.4229, Test Acc: 0.4409, F1 Score: 0.2706\n",
      "Epoch [10/30], Train Loss: 1.2490, Test Loss: 1.7109, Train Acc: 0.4511, Test Acc: 0.4373, F1 Score: 0.2752\n",
      "Epoch [11/30], Train Loss: 1.2017, Test Loss: 1.2854, Train Acc: 0.4511, Test Acc: 0.4445, F1 Score: 0.2786\n",
      "Epoch [12/30], Train Loss: 1.1844, Test Loss: 1.3183, Train Acc: 0.4631, Test Acc: 0.4511, F1 Score: 0.2924\n",
      "Epoch [13/30], Train Loss: 1.1545, Test Loss: 1.3895, Train Acc: 0.4481, Test Acc: 0.4607, F1 Score: 0.3123\n",
      "Epoch [14/30], Train Loss: 1.1261, Test Loss: 2.6641, Train Acc: 0.4769, Test Acc: 0.4409, F1 Score: 0.2706\n",
      "Epoch [15/30], Train Loss: 1.1961, Test Loss: 1.1816, Train Acc: 0.4691, Test Acc: 0.5033, F1 Score: 0.3891\n",
      "Epoch [16/30], Train Loss: 1.1232, Test Loss: 1.0987, Train Acc: 0.4847, Test Acc: 0.5309, F1 Score: 0.4313\n",
      "Epoch [17/30], Train Loss: 1.0737, Test Loss: 1.0924, Train Acc: 0.4955, Test Acc: 0.5123, F1 Score: 0.3920\n",
      "Epoch [18/30], Train Loss: 1.0766, Test Loss: 1.0387, Train Acc: 0.4907, Test Acc: 0.5393, F1 Score: 0.4438\n",
      "Epoch [19/30], Train Loss: 1.0692, Test Loss: 2.6485, Train Acc: 0.5087, Test Acc: 0.2639, F1 Score: 0.1169\n",
      "Epoch [20/30], Train Loss: 1.1393, Test Loss: 1.1365, Train Acc: 0.4955, Test Acc: 0.5117, F1 Score: 0.3921\n",
      "Epoch [21/30], Train Loss: 1.0814, Test Loss: 1.0591, Train Acc: 0.4919, Test Acc: 0.5489, F1 Score: 0.4455\n",
      "Epoch [22/30], Train Loss: 1.0740, Test Loss: 3.4073, Train Acc: 0.4973, Test Acc: 0.2741, F1 Score: 0.1443\n",
      "Epoch [23/30], Train Loss: 1.2162, Test Loss: 1.2560, Train Acc: 0.4823, Test Acc: 0.4853, F1 Score: 0.3951\n",
      "Epoch [24/30], Train Loss: 1.1133, Test Loss: 1.2051, Train Acc: 0.5033, Test Acc: 0.4691, F1 Score: 0.3755\n",
      "Epoch [25/30], Train Loss: 1.0751, Test Loss: 1.3299, Train Acc: 0.5117, Test Acc: 0.5075, F1 Score: 0.3849\n",
      "Epoch [26/30], Train Loss: 1.0566, Test Loss: 1.0569, Train Acc: 0.5129, Test Acc: 0.5435, F1 Score: 0.4273\n",
      "Epoch [27/30], Train Loss: 0.9977, Test Loss: 1.2301, Train Acc: 0.5123, Test Acc: 0.5009, F1 Score: 0.3736\n",
      "Epoch [28/30], Train Loss: 0.9622, Test Loss: 1.2154, Train Acc: 0.5405, Test Acc: 0.4535, F1 Score: 0.4327\n",
      "Epoch [29/30], Train Loss: 0.9546, Test Loss: 1.3744, Train Acc: 0.5321, Test Acc: 0.5009, F1 Score: 0.4060\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images'\n",
    "\n",
    "# Paths to the text files\n",
    "train_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_train.txt'\n",
    "test_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_test.txt'\n",
    "val_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_trainval.txt'\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"Boeing\": 0,\n",
    "    \"Airbus\": 1,\n",
    "    \"ATR\": 2,\n",
    "    \"Antonov\": 3,\n",
    "    \"BritishAerospace\": 4,\n",
    "    \"Beechcraft\": 5,\n",
    "    \"LockheedCorporation\": 6,\n",
    "    \"DouglasAircraftCompany\": 7,\n",
    "    \"Canadair\": 8,\n",
    "    \"Cessna\": 9,\n",
    "    \"McDonnellDouglas\": 10,\n",
    "    \"deHavilland\": 11,\n",
    "    \"Robin\": 12,\n",
    "    \"Dornier\": 13,\n",
    "    \"Embraer\": 14,\n",
    "    \"Eurofighter\": 15,\n",
    "    \"LockheedMartin\": 16,\n",
    "    \"DassaultAviation\": 17,\n",
    "    \"Fokker\": 18,\n",
    "    \"BombardierAerospace\": 19,\n",
    "    \"GulfstreamAerospace\": 20,\n",
    "    \"Ilyushin\": 21,\n",
    "    \"Fairchild\": 22,\n",
    "    \"Piper\": 23,\n",
    "    \"CirrusAircraft\": 24,\n",
    "    \"Saab\": 25,\n",
    "    \"Supermarine\": 26,\n",
    "    \"Panavia\": 27,\n",
    "    \"Tupolev\": 28,\n",
    "    \"Yakovlev\": 29\n",
    "}\n",
    "\n",
    "def select_balanced_classes(train_file, val_file, test_file, label_mapping, num_classes=5):\n",
    "    def count_labels(file_path):\n",
    "        counts = Counter()\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                _, label = line.strip().split(maxsplit=1)\n",
    "                if label in label_mapping:\n",
    "                    counts[label] += 1\n",
    "        return counts\n",
    "\n",
    "    train_counts = count_labels(train_file)\n",
    "    val_counts = count_labels(val_file)\n",
    "    test_counts = count_labels(test_file)\n",
    "\n",
    "    # Combine counts for all datasets\n",
    "    combined_counts = {label: train_counts[label] + val_counts[label] + test_counts[label] for label in label_mapping}\n",
    "    \n",
    "    # Sort by total count and select top labels\n",
    "    selected_labels = dict(sorted(combined_counts.items(), key=lambda item: item[1], reverse=True)[:num_classes])\n",
    "    \n",
    "    # Map selected labels to indices\n",
    "    selected_labels = {label: idx for idx, (label, _) in enumerate(selected_labels.items())}\n",
    "\n",
    "    return selected_labels\n",
    "\n",
    "# Intelligently select classes based on data distribution\n",
    "selected_labels = select_balanced_classes(train_label_file, val_label_file, test_label_file, label_mapping)\n",
    "\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomRotation(5),    # Random rotation up to 15 degrees\n",
    "    transforms.Resize((256, 256)),  # Resize to 128x128\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize([0.5], [0.5])  # Normalize\n",
    "])\n",
    "\n",
    "def filter_data(label_file, selected_labels):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            filename, label = parts\n",
    "            if label in selected_labels:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        image_tensor = transform(image)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(selected_labels[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Process training, validation, and test data for selected labels\n",
    "train_image_tensor, train_label_tensor, train_labels = filter_data(train_label_file, selected_labels)\n",
    "val_image_tensor, val_label_tensor, val_labels = filter_data(val_label_file, selected_labels)\n",
    "test_image_tensor, test_label_tensor, test_labels = filter_data(test_label_file, selected_labels)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training images: {len(train_image_tensor)}\")\n",
    "print(f\"Number of validation images: {len(val_image_tensor)}\")\n",
    "print(f\"Number of test images: {len(test_image_tensor)}\")\n",
    "\n",
    "# Print counts for each class in training, validation, and test datasets\n",
    "train_counts = Counter(train_labels)\n",
    "val_counts = Counter(val_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "print(\"Training dataset class distribution:\")\n",
    "for label, count in train_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Validation dataset class distribution:\")\n",
    "for label, count in val_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Test dataset class distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_image_tensor, train_label_tensor)\n",
    "val_dataset = TensorDataset(val_image_tensor, val_label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n",
    "\n",
    "# Example usage of DataLoader\n",
    "batch_size = 32  # Decreased batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"DataLoaders for selected labels created successfully.\")\n",
    "\n",
    "# Define a simplified CNN model\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(64)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.fc1 = nn.Linear(64 * 32 * 32, 128)  # Updated input size\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, len(selected_labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = x.view(-1, 64 * 32 * 32)  # Updated reshaping size\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "# Instantiate the simplified model\n",
    "model = SimpleCNN()\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)  # Fixed learning rate\n",
    "\n",
    "# Lists to store metrics\n",
    "train_losses, test_losses = [], []\n",
    "train_accuracies, test_accuracies = [], []\n",
    "f1_scores = []\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 30  # Decreased number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    # Training phase\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_test += labels.size(0)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    # Store metrics\n",
    "    train_losses.append(running_loss / len(train_loader))\n",
    "    test_losses.append(test_loss / len(test_loader))\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, Test Loss: {test_loss/len(test_loader):.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}, F1 Score: {f1:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")\n",
    "\n",
    "# Plotting results\n",
    "epochs = range(1, num_epochs + 1)\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_losses, label='Training Loss')\n",
    "plt.plot(epochs, test_losses, label='Test Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Test Loss vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, train_accuracies, label='Training Accuracy')\n",
    "plt.plot(epochs, test_accuracies, label='Test Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training and Test Accuracy vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot F1 Score\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(epochs, f1_scores, label='F1 Score')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('F1 Score')\n",
    "plt.title('F1 Score vs Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
