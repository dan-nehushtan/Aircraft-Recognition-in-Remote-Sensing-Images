{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8a3dc08e-2406-4f64-9498-0a58b6842883",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training images: 1667\n",
      "Number of validation images: 3333\n",
      "Number of test images: 1667\n",
      "Training dataset class distribution:\n",
      "Boeing: 733\n",
      "Airbus: 434\n",
      "Canadair: 134\n",
      "Cessna: 133\n",
      "Embraer: 233\n",
      "Validation dataset class distribution:\n",
      "Boeing: 1466\n",
      "Airbus: 867\n",
      "Canadair: 267\n",
      "Cessna: 266\n",
      "Embraer: 467\n",
      "Test dataset class distribution:\n",
      "Boeing: 734\n",
      "Airbus: 433\n",
      "Canadair: 133\n",
      "Cessna: 134\n",
      "Embraer: 233\n",
      "DataLoaders for selected labels created successfully.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'precision_score' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 268\u001b[0m\n\u001b[1;32m    266\u001b[0m test_accuracy \u001b[38;5;241m=\u001b[39m correct_test \u001b[38;5;241m/\u001b[39m total_test\n\u001b[1;32m    267\u001b[0m f1 \u001b[38;5;241m=\u001b[39m f1_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m--> 268\u001b[0m precision \u001b[38;5;241m=\u001b[39m precision_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    269\u001b[0m recall \u001b[38;5;241m=\u001b[39m recall_score(true_labels, predicted_labels, average\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    271\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(test_loss)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'precision_score' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn.init as init\n",
    "from torch.optim.lr_scheduler import StepLR, ReduceLROnPlateau\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import f1_score\n",
    "from collections import Counter\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n",
    "\n",
    "\n",
    "# Define the folder containing the images\n",
    "image_folder = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images'\n",
    "\n",
    "# Paths to the text files\n",
    "train_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_train.txt'\n",
    "test_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_test.txt'\n",
    "val_label_file = '/Users/louieburns/Library/CloudStorage/OneDrive-UniversityofLeeds/Year 3/AI and Machine Learning/Term 1/Coursework 1/Actual Coursework/dataoriginal/images_manufacturer_trainval.txt'\n",
    "\n",
    "# Define label mapping\n",
    "label_mapping = {\n",
    "    \"Boeing\": 0,\n",
    "    \"Airbus\": 1,\n",
    "    \"ATR\": 2,\n",
    "    \"Antonov\": 3,\n",
    "    \"BritishAerospace\": 4,\n",
    "    \"Beechcraft\": 5,\n",
    "    \"LockheedCorporation\": 6,\n",
    "    \"DouglasAircraftCompany\": 7,\n",
    "    \"Canadair\": 8,\n",
    "    \"Cessna\": 9,\n",
    "    \"McDonnellDouglas\": 10,\n",
    "    \"deHavilland\": 11,\n",
    "    \"Robin\": 12,\n",
    "    \"Dornier\": 13,\n",
    "    \"Embraer\": 14,\n",
    "    \"Eurofighter\": 15,\n",
    "    \"LockheedMartin\": 16,\n",
    "    \"DassaultAviation\": 17,\n",
    "    \"Fokker\": 18,\n",
    "    \"BombardierAerospace\": 19,\n",
    "    \"GulfstreamAerospace\": 20,\n",
    "    \"Ilyushin\": 21,\n",
    "    \"Fairchild\": 22,\n",
    "    \"Piper\": 23,\n",
    "    \"CirrusAircraft\": 24,\n",
    "    \"Saab\": 25,\n",
    "    \"Supermarine\": 26,\n",
    "    \"Panavia\": 27,\n",
    "    \"Tupolev\": 28,\n",
    "    \"Yakovlev\": 29\n",
    "}\n",
    "\n",
    "def select_balanced_classes(train_file, val_file, test_file, label_mapping, num_classes=5):\n",
    "    def count_labels(file_path):\n",
    "        counts = Counter()\n",
    "        with open(file_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                _, label = line.strip().split(maxsplit=1)\n",
    "                if label in label_mapping:\n",
    "                    counts[label] += 1\n",
    "        return counts\n",
    "\n",
    "    train_counts = count_labels(train_file)\n",
    "    val_counts = count_labels(val_file)\n",
    "    test_counts = count_labels(test_file)\n",
    "\n",
    "    # Combine counts for all datasets\n",
    "    combined_counts = {label: train_counts[label] + val_counts[label] + test_counts[label] for label in label_mapping}\n",
    "    \n",
    "    # Sort by total count and select top labels\n",
    "    selected_labels = dict(sorted(combined_counts.items(), key=lambda item: item[1], reverse=True)[:num_classes])\n",
    "    \n",
    "    # Map selected labels to indices\n",
    "    selected_labels = {label: idx for idx, (label, _) in enumerate(selected_labels.items())}\n",
    "\n",
    "    return selected_labels\n",
    "\n",
    "# Intelligently select classes based on data distribution\n",
    "selected_labels = select_balanced_classes(train_label_file, val_label_file, test_label_file, label_mapping)\n",
    "\n",
    "# Define transformations for image processing\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),  # Random horizontal flip\n",
    "    transforms.RandomRotation(10),    # Random rotation up to 10 degrees\n",
    "    transforms.Resize((256, 256)),  # Resize to 256x256\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),  # Data augmentation\n",
    "    transforms.ToTensor(),          # Convert to tensor\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # Normalize for RGB\n",
    "])\n",
    "\n",
    "def filter_data(label_file, selected_labels):\n",
    "    image_data = []\n",
    "    labels = []\n",
    "\n",
    "    with open(label_file, \"r\") as f:\n",
    "        for line in f:\n",
    "            parts = line.strip().split(maxsplit=1)\n",
    "            if len(parts) != 2:\n",
    "                continue  # Skip malformed lines\n",
    "            filename, label = parts\n",
    "            if label in selected_labels:\n",
    "                image_path = os.path.join(image_folder, filename + \".jpg\")\n",
    "                try:\n",
    "                    if os.path.exists(image_path):\n",
    "                        image = Image.open(image_path).convert(\"RGB\")\n",
    "                        image_tensor = transform(image)\n",
    "                        image_data.append(image_tensor)\n",
    "                        labels.append(selected_labels[label])\n",
    "                except Exception as e:\n",
    "                    print(f\"Error processing {image_path}: {e}\")\n",
    "\n",
    "    # Convert lists to PyTorch tensors\n",
    "    image_tensor = torch.stack(image_data)\n",
    "    label_tensor = torch.tensor(labels, dtype=torch.long)\n",
    "\n",
    "    return image_tensor, label_tensor, labels\n",
    "\n",
    "# Process training, validation, and test data for selected labels\n",
    "train_image_tensor, train_label_tensor, train_labels = filter_data(train_label_file, selected_labels)\n",
    "val_image_tensor, val_label_tensor, val_labels = filter_data(val_label_file, selected_labels)\n",
    "test_image_tensor, test_label_tensor, test_labels = filter_data(test_label_file, selected_labels)\n",
    "\n",
    "# Print dataset sizes\n",
    "print(f\"Number of training images: {len(train_image_tensor)}\")\n",
    "print(f\"Number of validation images: {len(val_image_tensor)}\")\n",
    "print(f\"Number of test images: {len(test_image_tensor)}\")\n",
    "\n",
    "# Print counts for each class in training, validation, and test datasets\n",
    "train_counts = Counter(train_labels)\n",
    "val_counts = Counter(val_labels)\n",
    "test_counts = Counter(test_labels)\n",
    "\n",
    "print(\"Training dataset class distribution:\")\n",
    "for label, count in train_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Validation dataset class distribution:\")\n",
    "for label, count in val_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "print(\"Test dataset class distribution:\")\n",
    "for label, count in test_counts.items():\n",
    "    print(f\"{list(selected_labels.keys())[list(selected_labels.values()).index(label)]}: {count}\")\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(train_image_tensor, train_label_tensor)\n",
    "val_dataset = TensorDataset(val_image_tensor, val_label_tensor)\n",
    "test_dataset = TensorDataset(test_image_tensor, test_label_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d2c3e8-2032-46b9-a84b-dccca9f66e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataLoaders for selected labels created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Example usage of DataLoader\n",
    "batch_size = 32  # Increased batch size\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "print(\"DataLoaders for selected labels created successfully.\")\n",
    "# Improved CNN Model\n",
    "class ImprovedCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ImprovedCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, padding=1)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.dropout = nn.Dropout(0.3)  # Adjusted dropout\n",
    "        self.fc1 = nn.Linear(256 * 16 * 16, 512)\n",
    "        self.fc2 = nn.Linear(512, len(selected_labels))\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        # Weight initialisation\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(self.relu(self.bn1(self.conv1(x))))\n",
    "        x = self.pool(self.relu(self.bn2(self.conv2(x))))\n",
    "        x = self.pool(self.relu(self.bn3(self.conv3(x))))\n",
    "        x = self.pool(self.relu(self.bn4(self.conv4(x))))\n",
    "        x = x.view(-1, 256 * 16 * 16)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "                if m.bias is not None:\n",
    "                    nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                init.xavier_uniform_(m.weight)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "# Updated Data Augmentation\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomRotation(15),\n",
    "    transforms.RandomAffine(10, translate=(0.1, 0.1)),\n",
    "    transforms.RandomPerspective(distortion_scale=0.2, p=0.5),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ColorJitter(brightness=0.3, contrast=0.3, saturation=0.3, hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
    "])\n",
    "\n",
    "# Instantiate model, criterion, optimizer, and scheduler\n",
    "model = ImprovedCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)\n",
    "\n",
    "# Early Stopping Parameters\n",
    "early_stopping_patience = 10\n",
    "best_f1 = 0\n",
    "epochs_without_improvement = 0\n",
    "\n",
    "# Training Loop with Early Stopping and Learning Rate Scheduling\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    for images, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "        total_train += labels.size(0)\n",
    "\n",
    "    train_accuracy = correct_train / total_train\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    test_loss = 0.0\n",
    "    correct_test = 0\n",
    "    total_test = 0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct_test += (predicted == labels).sum().item()\n",
    "            total_test += labels.size(0)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(predicted.cpu().numpy())\n",
    "\n",
    "    test_accuracy = correct_test / total_test\n",
    "    f1 = f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    precision = precision_score(true_labels, predicted_labels, average='weighted')\n",
    "    recall = recall_score(true_labels, predicted_labels, average='weighted')\n",
    "\n",
    "    scheduler.step(test_loss)\n",
    "\n",
    "    # Early stopping check\n",
    "    if f1 > best_f1:\n",
    "        best_f1 = f1\n",
    "        epochs_without_improvement = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_model.pth')\n",
    "    else:\n",
    "        epochs_without_improvement += 1\n",
    "\n",
    "    if epochs_without_improvement >= early_stopping_patience:\n",
    "        print(\"Early stopping triggered.\")\n",
    "        break\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Train Loss: {running_loss/len(train_loader):.4f}, \"\n",
    "          f\"Test Loss: {test_loss/len(test_loader):.4f}, Train Acc: {train_accuracy:.4f}, \"\n",
    "          f\"Test Acc: {test_accuracy:.4f}, F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "print(\"Training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
